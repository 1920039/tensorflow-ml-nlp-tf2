{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.4.1-py3-none-any.whl (475 kB)\n",
      "\u001b[K     |████████████████████████████████| 475 kB 511 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from transformers) (1.16.0)\n",
      "Collecting tokenizers==0.0.11\n",
      "  Downloading tokenizers-0.0.11-cp36-cp36m-macosx_10_13_x86_64.whl (869 kB)\n",
      "\u001b[K     |████████████████████████████████| 869 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\n",
      "\u001b[K     |████████████████████████████████| 860 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from transformers) (4.31.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from transformers) (2019.6.8)\n",
      "Requirement already satisfied: boto3 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from transformers) (1.9.92)\n",
      "Requirement already satisfied: sentencepiece in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from transformers) (0.1.82)\n",
      "Requirement already satisfied: six in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from requests->transformers) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from boto3->transformers) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.92 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from boto3->transformers) (1.12.186)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from boto3->transformers) (0.9.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.92->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/chojunghyun/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.92->boto3->transformers) (2.7.5)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=fe5a14dc6a61800496594ca11c6ced2a31e35295c4c72488bc3b822647af81f4\n",
      "  Stored in directory: /Users/chojunghyun/Library/Caches/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, joblib, sacremoses, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 joblib-0.14.1 sacremoses-0.0.38 tokenizers-0.0.11 transformers-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0209 16:29:53.589794 4543352256 file_utils.py:38] PyTorch version 1.1.0.post2 available.\n",
      "I0209 16:29:57.024883 4543352256 file_utils.py:54] TensorFlow version 2.0.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0209 16:31:46.053390 4543352256 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/config.json from cache at /Users/chojunghyun/.cache/torch/transformers/062a4aa346f4830513250d6b61df96193c3781f3c6abdf2f78d619dc2fb82a8b.21d07ff460263e310cb41ad9356620f00f07bddf2c0fe0add70a82d3f9c307a7\n",
      "I0209 16:31:46.054266 4543352256 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "I0209 16:31:46.886284 4543352256 modeling_tf_utils.py:338] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tf_model.h5 from cache at None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1a6b9dad8e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'monologg/kobert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build the network with dummy inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Error retrieving file {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;31m# 'by_name' allow us to do transfer learning by skipping/adding layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('monologg/kobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
