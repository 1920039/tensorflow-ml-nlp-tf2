{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "PATH = 'data_in/ChatBotData.csv_short'\n",
    "VOCAB_PATH = 'data_in/vocabulary.txt'\n",
    "\n",
    "MAX_SEQUENCE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv(path, header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = load_data(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이징과 어휘사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을\n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진\n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def load_vocabulary(path, vocab_path):\n",
    "    # 사전을 담을 배열 준비한다.\n",
    "    vocabulary_list = []\n",
    "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
    "    # 그 파일의 존재 유무를 확인한다.\n",
    "    if not os.path.exists(vocab_path):\n",
    "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
    "        # 데이터를 가지고 만들어야 한다.\n",
    "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
    "        # 데이터 파일의 존재 유무를 확인한다.\n",
    "        if (os.path.exists(path)):\n",
    "            # 데이터가 존재하니 판단스를 통해서\n",
    "            # 데이터를 불러오자\n",
    "            data_df = pd.read_csv(path, encoding='utf-8')\n",
    "            # 판다스의 데이터 프레임을 통해서\n",
    "            # 질문과 답에 대한 열을 가져 온다.\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "#             if DEFINES.tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "#                 question = prepro_like_morphlized(question)\n",
    "#                 answer = prepro_like_morphlized(answer)\n",
    "            data = []\n",
    "            # 질문과 답변을 extend을\n",
    "            # 통해서 구조가 없는 배열로 만든다.\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            # 토큰나이져 처리 하는 부분이다.\n",
    "            words = data_tokenizer(data)\n",
    "            # 공통적인 단어에 대해서는 모두\n",
    "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
    "            # set해주고 이것들을 리스트로 만들어 준다.\n",
    "            words = list(set(words))\n",
    "            # 데이터 없는 내용중에 MARKER를 사전에\n",
    "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
    "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
    "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNWON>\"\n",
    "            words[:0] = MARKER\n",
    "        # 사전을 리스트로 만들었으니 이 내용을\n",
    "        # 사전 파일을 만들어 넣는다.\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서\n",
    "    # 그 파일을 불러서 배열에 넣어 준다.\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    # 배열에 내용을 키와 값이 있는\n",
    "    # 딕셔너리 구조로 만든다.\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return char2idx, idx2char, len(char2idx)\n",
    "\n",
    "\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    # 리스트를 키가 단어이고 값이 인덱스인\n",
    "    # 딕셔너리를 만든다.\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    # 리스트를 키가 인덱스이고 값이 단어인\n",
    "    # 딕셔너리를 만든다.\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    # 두개의 딕셔너리를 넘겨 준다.\n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다.)\n",
    "    sequences_input_index = []\n",
    "    # 하나의 인코딩 되는 문장의\n",
    "    # 길이를 가지고 있다.(누적된다.)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "#     if DEFINES.tokenize_as_morph:\n",
    "#         value = prepro_like_morphlized(value)\n",
    "\n",
    "    print(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 인코딩 할때\n",
    "        # 가지고 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로\n",
    "        # 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_input_index에 넣어 준다.\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과\n",
    "    # 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_output_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_output_index = []\n",
    "    # 하나의 디코딩 입력 되는 문장의\n",
    "    # 길이를 가지고 있다.(누적된다)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "#     if DEFINES.tokenize_as_morph:\n",
    "#         value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 디코딩 할때 가지고\n",
    "        # 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로\n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
    "        # 값인 인덱스를 넣어 준다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_output_index 넣어 준다.\n",
    "        sequences_output_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_target_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_target_index = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "#     if DEFINES.tokenize_as_morph:\n",
    "#         value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_target_index에 넣어 준다.\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다.', '가스비 비싼데 감기 걸리겠어', '남자친구 교회 데려가고 싶어', '남자친구 또 운동 갔어', '남자친구 생일인데 뭘 줄까', '남자친구 승진 선물로 뭐가 좋을까?', '남자친구 오늘 따라 훈훈해 보인다', '남자친구 오늘 좀 질린다.', '남자친구가 나 안 믿어줘', '남자친구가 너무 바빠', '남자친구가 너무 운동만 해', '남자친구가 너무 잘생겼어']\n"
     ]
    }
   ],
   "source": [
    "index_inputs, input_seq_len = enc_processing(inputs, char2idx)\n",
    "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx)\n",
    "index_targets = dec_target_processing(outputs, char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# Show length\n",
    "print(len(index_inputs), len(input_seq_len), len(index_outputs), len(output_seq_len), len(index_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(index_inputs)\n",
    "BATCH_SIZE = 5\n",
    "steps_per_epoch = len(index_inputs)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((index_inputs, index_outputs)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5, 10]), TensorShape([5, 10]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (5, 10, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (5, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (5, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (5, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (5, 111)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(tf.keras.Model):\n",
    "    def __init__(self, inp, targ, enc_hidden):\n",
    "        super(seq2seq, self).__init__()\n",
    "\n",
    "    def call(self, inp, targ, enc_hidden):\n",
    "        loss = 0\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([STD_INDEX] * BATCH_SIZE, 1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.5075\n",
      "Epoch 1 Loss 1.6713\n",
      "Time taken for 1 epoch 14.022367000579834 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5797\n",
      "Epoch 2 Loss 1.5944\n",
      "Time taken for 1 epoch 0.923520565032959 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6217\n",
      "Epoch 3 Loss 1.4673\n",
      "Time taken for 1 epoch 0.7896945476531982 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3807\n",
      "Epoch 4 Loss 1.3910\n",
      "Time taken for 1 epoch 0.9296953678131104 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.3133\n",
      "Epoch 5 Loss 1.3385\n",
      "Time taken for 1 epoch 0.7866320610046387 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.1275\n",
      "Epoch 6 Loss 1.2493\n",
      "Time taken for 1 epoch 0.9295482635498047 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9240\n",
      "Epoch 7 Loss 1.2195\n",
      "Time taken for 1 epoch 0.7831146717071533 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.9312\n",
      "Epoch 8 Loss 1.1320\n",
      "Time taken for 1 epoch 0.9414920806884766 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.8620\n",
      "Epoch 9 Loss 1.0604\n",
      "Time taken for 1 epoch 0.7890908718109131 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.8115\n",
      "Epoch 10 Loss 1.0307\n",
      "Time taken for 1 epoch 0.9271271228790283 sec\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "!apt-get update -qq #나눔고딕 인스톨\n",
    "!apt-get install fonts-nanum* -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanumGothic Eco\n"
     ]
    }
   ],
   "source": [
    "path = '/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf'  \n",
    "font_name = fm.FontProperties(fname=path, size=10).get_name() \n",
    "print(font_name)\n",
    "plt.rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((MAX_SEQUENCE, MAX_SEQUENCE))\n",
    "    \n",
    "    index_inputs, input_seq_len = enc_processing([sentence], char2idx)    \n",
    "    \n",
    "    inputs = index_inputs\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([char2idx[STD]], 0)\n",
    "\n",
    "    for t in range(MAX_SEQUENCE):    \n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += idx2char[predicted_id] + ' '\n",
    "\n",
    "        if idx2char[predicted_id] == '<END>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(attention, sentence, result):\n",
    "    #result = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence)) # 텍스트 처리\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2d43ba2090>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['남자친구 승진 선물로 뭐가 좋을까?']\n",
      "Input: 남자친구 승진 선물로 뭐가 좋을까?\n",
      "Predicted translation: 평소에 것 나라를 구하셨나요 나라를 구하셨나요 나라를 구하셨나요 나라를 구하셨나요 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "query = \"남자친구 승진 선물로 뭐가 좋을까?\"\n",
    "\n",
    "result, attention = evaluate(query)\n",
    "translate(attention, query, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
