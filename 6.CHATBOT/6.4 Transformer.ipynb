{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "PATH = 'data_in/ChatBotData.csv_short'\n",
    "VOCAB_PATH = 'data_in/vocabulary.txt'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQUENCE = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv(path, header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = load_data(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이징과 어휘사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을\n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진\n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def load_vocabulary(path, vocab_path):\n",
    "    # 사전을 담을 배열 준비한다.\n",
    "    vocabulary_list = []\n",
    "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
    "    # 그 파일의 존재 유무를 확인한다.\n",
    "    if not os.path.exists(vocab_path):\n",
    "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
    "        # 데이터를 가지고 만들어야 한다.\n",
    "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
    "        # 데이터 파일의 존재 유무를 확인한다.\n",
    "        if (os.path.exists(path)):\n",
    "            # 데이터가 존재하니 판단스를 통해서\n",
    "            # 데이터를 불러오자\n",
    "            data_df = pd.read_csv(path, encoding='utf-8')\n",
    "            # 판다스의 데이터 프레임을 통해서\n",
    "            # 질문과 답에 대한 열을 가져 온다.\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "#             if DEFINES.tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "#                 question = prepro_like_morphlized(question)\n",
    "#                 answer = prepro_like_morphlized(answer)\n",
    "            data = []\n",
    "            # 질문과 답변을 extend을\n",
    "            # 통해서 구조가 없는 배열로 만든다.\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            # 토큰나이져 처리 하는 부분이다.\n",
    "            words = data_tokenizer(data)\n",
    "            # 공통적인 단어에 대해서는 모두\n",
    "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
    "            # set해주고 이것들을 리스트로 만들어 준다.\n",
    "            words = list(set(words))\n",
    "            # 데이터 없는 내용중에 MARKER를 사전에\n",
    "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
    "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
    "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNWON>\"\n",
    "            words[:0] = MARKER\n",
    "        # 사전을 리스트로 만들었으니 이 내용을\n",
    "        # 사전 파일을 만들어 넣는다.\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서\n",
    "    # 그 파일을 불러서 배열에 넣어 준다.\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    # 배열에 내용을 키와 값이 있는\n",
    "    # 딕셔너리 구조로 만든다.\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return char2idx, idx2char, len(char2idx)\n",
    "\n",
    "\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    # 리스트를 키가 단어이고 값이 인덱스인\n",
    "    # 딕셔너리를 만든다.\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    # 리스트를 키가 인덱스이고 값이 단어인\n",
    "    # 딕셔너리를 만든다.\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    # 두개의 딕셔너리를 넘겨 준다.\n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다.)\n",
    "    sequences_input_index = []\n",
    "    # 하나의 인코딩 되는 문장의\n",
    "    # 길이를 가지고 있다.(누적된다.)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "#     if DEFINES.tokenize_as_morph:\n",
    "#         value = prepro_like_morphlized(value)\n",
    "\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 인코딩 할때\n",
    "        # 가지고 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로\n",
    "        # 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_input_index에 넣어 준다.\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과\n",
    "    # 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_output_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_output_index = []\n",
    "    # 하나의 디코딩 입력 되는 문장의\n",
    "    # 길이를 가지고 있다.(누적된다)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "#     if DEFINES.tokenize_as_morph:\n",
    "#         value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 디코딩 할때 가지고\n",
    "        # 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로\n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
    "        # 값인 인덱스를 넣어 준다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_output_index 넣어 준다.\n",
    "        sequences_output_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_target_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_target_index = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "#     if DEFINES.tokenize_as_morph:\n",
    "#         value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_target_index에 넣어 준다.\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inputs, input_seq_len = enc_processing(inputs, char2idx)\n",
    "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx)\n",
    "index_targets = dec_target_processing(outputs, char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(index_inputs)\n",
    "\n",
    "# def mapping_fn(enc_input, dec_input, labels=None):\n",
    "#     features = {\"enc_input\": enc_input, \"dec_input\": dec_input}\n",
    "    \n",
    "#     if labels is not None:\n",
    "#         return features, labels\n",
    "#     else:\n",
    "#         return features\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((index_inputs, index_outputs, index_targets)).shuffle(BUFFER_SIZE)\n",
    "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# dataset = dataset.map(mapping_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydeXxU1d3/3+femclMJslkT0gChFUW2RRFxH0XFbXWrVr3olX7VOuvrbZP9anWWp9Wq4/Vtm51ad2XCopVRBZBERDZd8KaQPY9s917z++PuTOZhCQMkADB8369zmvufs8M4cydz/d8P18hpUShUCgU3w20Q90BhUKhUBw81KCvUCgU3yHUoK9QKBTfIdSgr1AoFN8h1KCvUCgU3yHUoK9QKBTfIXp00BdCbBVCrBRCLBNCLLG3ZQohZgohNtqvGT3ZB4VCoThUCCFeFEJUCCFWdbJfCCH+TwixSQixQghxTNy+6+1xcqMQ4vru6tPBeNI/XUo5Vko53l6/F5glpRwCzLLXFQqF4kjkJeC8LvafDwyx21TgrxB5OAYeACYAxwMPdNcD8qGQdy4GXraXXwYuOQR9UCgUih5HSjkPqOnikIuBV2SEhUC6EKIPcC4wU0pZI6WsBWbS9ZdHwji64yJdIIFPhRAS+LuU8lkgT0q5y96/G8jr6EQhxFQi33x4kz3Htshkxg7vz7qyBvq3lBNuCdNcPIiKXZWMHVpA6bdryMn1stOTR31lFQMHFODYVoK3MJf1jQ5aaqvJKcijUNZTtqUStybIHlbMlmZBbUU1utOFI8lDsLEWp9dHv7wU0kL11G8pp96w8GiC9NwUnH36UtYYpq4hQF5mMllJYFTuprmikUbDAiBZ1/CmJ5GUk4Xp8bHr29Uk6xqeFCfuzFREaiZBHNQFDOqaQ4QDYYyQH2kaICXHDMrBam4g1NhCuDlEKGQRtCSmlFiAAHQBDiHIyE3BCIQxgwZG2CJkH2dKsKKfpd3SRg4jZEpChkXIMAkZFpYpI82ykJZpt8jy2AIP6E6E7kBqOmgaCB0TsKzIP64pJaYl2bhlF0IIEAKB/appreuahhAaQgicSTpSgpQS7Fcpgegr0X0SiSQl1Y0QAgFoQmDfBoFAE0T22dvKSqsBSSzPvH3Gedz6oAF9EHGfEcJ+jfQ4bj3C2k079/oHH+XoIX073C7EnttWrt+e8HUBRg/r1/G1O9i2fF3i1x7byXU7Ytk+XDdy7f77cO1tiV93eNvrLlu7DemvrpJS5iR8kXZoaUUSI5DQsdJfvRqIP/hZe5xLlEJgR9z6TntbZ9sPmJ4e9E+SUpYKIXKBmUKIdfE7pZTS/kLYA/uDexbg2NEj5Up9AgsWPMPEBz/n2aV/YvfyCr564i3+75Hn+PLjB/h15jH8+Krj+cWYu/nw6ed5/NWHyP7xFYz//Z2cOjeLb97+F1fdfzePhD7ktz98jqEpLm545zl+uMjN+0+/REp+MbmDhrFx9vv0OfZc/nzPaZxdOoNPfvgYM3Y3cXRKEhfffBK59z7OQ1+U8e6nG7nn6jFcN1Cn6m8P8/Vf5jG7sgWAY3xuJlw0hEFTr6fx6PP5XcZIxvjcjDm5L0OvPAXX6VezWeTw7zXlfLhoJzvW76Ju6yoC9ZVIy2TR21Np+fpTSucuo2xxKdu2N7C1JUxNyCRkSXQBPqdOtkvn8ptPpnrtTmo21lK5u4lSv0Ft2KTe/gKAyBeESxOc886nbK8PsLWqmW3VzZRVt9DcEKSlPkigJUSwsY5QSz2Gvwkj0MyX949Ez8hF82VjeXxY7lQst48WQ9IStvAbkuawSX3A4PxrH0R3utAcLjSHE83hQk/yoDtcsWXN4cLhclI4KBMjbGGETIywiRG2MA0L07Sw7FfTsLCMENIymXjaUbgcGi6HHnnVNZIcmr2tbfvNAy/FvrgApGW2ebXsV4Bn/vErNAG6EGhCoGuRL5X260KAhuDYKb+IXWdvTP/0z0DrIB/9SS3sDVrcCN3/9J8kdM0os+b+pcMBXutgY+7JdyZ83bnzn26z3tE9omROuiPh6wLMX/BMwsemn3h7wscuaHdd38TbCS/7R+LfGh1hBHAcNSWhQ8PL/hGIk657BT0q70gpS+3XCuB9ItpUuf3zBfu1oif7oFAoFPuEEAhNT6h1A6VA/M/CIntbZ9sPmB4b9IUQXiFEanQZOAdYBUwDopHo64EPeqoPCoVCse8I+xfr3ls3MA24zp7FcwJQb8vfnwDnCCEy7ADuOfa2A6Yn5Z084H3756wDeE1K+R8hxGLgLSHEzcA24Ioe7INCoVDsG/aTfvdcSrwOnAZkCyF2EpmR4wSQUv4NmAFMBjYBLcCN9r4aIcRDwGL7Ug9KKbsKCCdMjw36UsoSYEwH26uBM/flWmsqQkz8+XXMGTaBxYun85PcEp7aNYurv/8Xbrz7RpacfxF5SQ6Sf/sCn055gKMnX8a5uz/l5/N3UJ91Kitm/IF+Ey/k0XMHMuuo1/GbkrNvncjX7hHM/fhdpGUy4pTjWPrBh7h9OZx82iDOyZesf/BtFtb4SXFojBuVS5/Lr2Te7hCfLNrBuHF9OGdwFtai19g6czUr64OELElfj5OBgzMoPGUsDJnAmko/2S4HRfkp5I4bgHvUROq9fVixpY7FW2qo3t1IS3UpoeZ6pGWiuzyESlZTt2EHdVtqqdvVRGXQpMlo1ehdmsCra2S6dJp3V9Nc3kJLlZ/6sEWTYeE3I8HcKLoQuDRBrT9MbUuI6uYQ1U0hgn6DkN8gFDQIB1owQ37MoD+mpQuPF+HxIp1JSEcS0uEmLCFkScKWJGRKAoZFwLAQevQnr4bQdDSnC83+Caw5XAhNR3c4EEJgGrJVw7ckliWRdmtdtoPKpomuCXRNi7wKYa930IRoo7l3pedL04x9Nono+ftCoro/dBzY3R860vPFAVy8m7rVKxGA0Ltn0JdSXr2X/RLoMEAipXwReLFbOhJHTwdyFQqFonchBFo3PekfjqhBX6FQKNrRXfLO4Yga9BUKhSKebtT0D0fUoK9QKBRxCASaw3mou9Fj9AqXzWBjHbPOCPDpzgbeG3ket148lNNe2oY3py9PDN7Nqwt2cPtz13HhY18gNJ13fjqJT675X7JdOnc9/RXSNPnNzcdR9ehdzCht4Py+afT52W/55dsrqNqwmNyRk/jNlJEEG2voN/5kfnHmEILT/8Y3n26hJmQyxpfEiGsmUT/wJF5ZtIPSNZu5anxfCpu3UPrx52xYVUl50MCjC0akuSg6sRjvhDPYraWzYFsNg1Oc5I/NJfPY0ZiFI9lSF+LbHXXs3NlAY0UlgdpyzJAfoek4PSkEtm6mblMZDTsbqAyaNBgmfjOSbOTSBCkODZ/TDuSWN9NS3UJtyKA+bBGwpJ2VG/nsdNGanFUTCFPREKS6KUjAHybkDxMKGhhhEzPoxwhFgriWEcYMh9CSUyHJi+VMRjrdWE53JKPXlHYw1yJoWARNKxa0jQZxRXwQV48GcwW6Q4skX5lWJHBr2gFcGQnuWnZwNxrElZYZC9S69EgCVjQxq30gV7MDl9HErM6IBnE7Cn52hhD7FqCNngNdJ2btD9/lIOtB4eDO0z/oqCd9hUKhaEdvHdATQQ36CoVCEY8Q3TZl83BEDfoKhUIRh+DIftLvFZp+3359+P2Jd3L/U1eytC5A6nPvsvjNf/LqH67ln2f8lIv7+5g15hZWfvgWF99yOe7n7mXG7iZ+cPMxbJk/jdEXTOHazEqmP/kFmS6dUx65nJe3SFbP+gKX18fZ5x3NaclVpBUN5erzhzIisJllf53J0roAOUk6Y88eQNqF1zBjYw2Ll5RSt30tpxf78M97ny2fbWZDUwhTQnGyi77H5NPn9BMw+h/LN2WNzF5bQdHQLPInDMM1ciIVMoVvd9WzeEsNNeVNtFSXEvY3AaC7PLhSM6jdsIP6bQ3UVPtjiVlRjT4+MSs500NzRTNNtQHqwxbNpoXftPZIzPLoGm5No6YpRE1ziLpoYlbQJBw0MPxNmCE/VtjW8+3kLM2bhnR5IolZTk+rnm+3lrBJS9gkYFhtjNaEpqPFJWVpDheaJtB1DV3XsIyInh9NzooarUkpW/X8uMSqqNGargkccRp+G11fCPQ4sburxKz4zyaRxKx9yXGK3q8zPT8elZh1mCI0dIcrodYbUU/6CoVCEY84sp/01aCvUCgUcQjUPH2FQqH4TnEkD/q9QtNPqy0l3+3gmaE3cf/LN3HaXW8x4vzvM/q937K0LsA5c1/hxw9NI3fEJF48N5eXf/cpp2QnU/THl0krGspLPzqeb+/4OcvrA1x8RjHNk+/msdeX01S+leITTue/zxrMrr/+iWEnTWDqcUWUvfA0C1ZUYErJpMI0Bl9zAZuS+vPS/C3sXrcCI9CEZ+MXbP7gK1Zsr6cmZJLp0hmW76XvaSNwjTudTQ2SORurKC2pJf/YQtLGHUc4fzjrqv0s2lpLVVkDTRWlBJtqsYwQQtNxedNIziqkblM5DTsb2B2IztFvNVpLcUT0fJ/bgTcvmebyZmpCpm20Zu0xR9+lRczWPLqIzdEP+g2C/jDhoEE4EMAMRebom/Y8/ej8eOn0IB1upCsZU3O20fMDYYuWcMRszR82Y0ZrUeO1mJ5vz9nXdA3NoSE0EZuTLy1i+n5HRmvR5b0ardlz9DVNtJmj39G8+ugc/Sid6fntOdC59e2vc7jq+Yeaw6Lrap6+QqFQfJdQ8o5CoVB8ZxBCoDl758ycRFCDvkKhUMSjDNcUCoXiu8WRPOj3ikDu7vImblz3Mb/71ZM8kn4ZNSXL+fpXk3js/o/5ydRj+cHsIFUbFvPUfZNZcuUPKQsYXPrcj7jm9RVcdcNk+s37G29/toXjMtyMe/x/uGf6WrZ++Qm+fsO587KjKVz7EV8/t5B7p4wgbdGbLH9xEVtbwgxNSWLk1cegnXYtr35TyqZlO2gq34rT66Ni+vuUzNvODn8YlyYYmuKi36QiMk4+jbr0QczfXsuS9ZXUlpaRf8LRiCHHsbUhzJKddazZUkvt7jpaqssw7MQsl9eH25dDamY6ddvqKW8MURtuDeLqAlIcGml2INeb5yUlz0tDSziWmNVRENejC9x2ALimOUhjc4hgIEzIbxAOGq1Ga3ZilmW1BlClKzmSnOX0EDAi5mohU2JYrRWz/HZyVrzBmt4uiKs7NHSHFgmUOiLJWZZpG6xJO4DbLjErvkWDtY4OArguhxZLzNJjhmttg7UdJWZBxwHbKPGJWYkGcdvft7uN1g4GvaCLBwVNEwm13kivGPQVCoXiYCGEQGiJtQSvd54QYr0QYpMQ4t4O9v9ZCLHMbhuEEHVx+8y4fdO64/0peUehUCjaoevd8zwshNCBp4GzgZ3AYiHENCnlmugxUsq7447/CTAu7hJ+KeXYbumMjXrSVygUingE3fmkfzywSUpZIqUMAW8AF3dx/NXA693wLjqlVwz6eVkexv1xNYXHnsmf7n+SX//uTuYddwZDU1zw238w/e//ZPzlV3Pepjf455xtXHfuQJaMvo5Zr03n8TNymXHny4QsyeSfn8ln4ihmvr8AgDFnT+TGYV5WPPIc86paODfLz6onX2NOZTM+p8YJEwoouPYGPt8Z4KP5W6nZtBSAjP5Hs2n6cpbXB/GbkgK3gyHDs+l71ngYNolvdzfz6erdlG+vo6l8K+5xp1KX3IelZQ0s2FhFVVkDzZXbCTXXRzRrlwdXSgbenCLSspOp29XE7oBBkxHR6QE8uhYzWvNluknJTcabn05NyLITs2TsWIjo2y5N4NY0UhyRVh01WvMbhIIG4UALZsiPGbSTsiwTKxxq1dNto7WwhJBdnCXeaK0lbBIwLQKGGdPwNa1tcpbucKDrkaSsSHIWMaM1abeuErOi76WzwinRpCrNTtDqymgtPjErEivo2mgtnr0lDXWm53dE/LUO5D/gkWa0dlgkZhF12ey2Qb8Q2BG3vtPetud9hegPDAA+j9vsFkIsEUIsFEJcsp9vqQ1K3lEoFIo2iC6D/O3IFkIsiVt/Vkr57H7e+CrgHSll/BNEfyllqRBiIPC5EGKllHLzfl4fUIO+QqFQtMWWdxKkSko5vov9pUDfuPUie1tHXAXcEb9BSllqv5YIIeYQ0fsPaNDvFfKOQqFQHEy6Ud5ZDAwRQgwQQriIDOx7zMIRQgwDMoCv4rZlCCGS7OVsYBKwpv25+0qveNL35/WndO6H1H7xBKN+BndVvMU966r5y5KnOer+T/Hm9mXWTyfyjz63Mzw1idGvvMzoh+YTqK9i009/xGcVzfzg+AJ8dz3Gvb+bTU3JcgacNIUnLxtN3XO/4bO52wGoeekx5s/dTpNhMTk/hVG3nMPOnHE88+5Kdq5aRbCxhpS8YvqPGsDqj2rYHTDwOTVGZXrof+YwkidOZnPYy6wNO9i8qZq6HRsI1FdiFI1mXYWfL0tqKNtRT8PuMvxxxdBdXh+ejHzSspIpyE+h1G/QYBuoQVujtewkB95cLykFqaQU5lAfNruYo6/h0SPnpjl1WppDhPxh22wthOFv2qMYepsCJq5kTD2JQNgiaNhGa4YV0/OD9tx9f8hsNVZzuCLNab/aer6ua7FCKkY4UjTFNK1YMXTTMPbQ86MtXst3tdP2tbg5+noX/wfb6/mwd6O1fZmj3xldzdHvbj2/N3O46PkQ6Yvu6J4OSSkNIcSdwCeADrwopVwthHgQWCKljH4BXAW8IWVcBSQYDvxdCGER+XP5Q/ysn/2lVwz6CoVCcTDpTqdSKeUMYEa7bfe3W/+fDs77EhjVbR2xUYO+QqFQxCFE7822TQQ16CsUCkU79iGQ2+tQg75CoVC0Qw36h5it23bzwPt3M2fYBJatWcjDGT/nv344its3F7Lj6//jhRceZPUVU1jVEOB//3ULUz+toGTeB4y68Ape/+MdjPG5OfFv93P79HWs//xj0oqGcvtVoxm6/XM+fmwWW1vCnJ6TzJKn5rK2McjQFBejrzsWx3k/4uVFpaxctJ2GnRtwen3kDR/LRRP6sqEpiC5gaIqLAaf3J/esM2nIHcHcNZV8saqcyi07aakqQ1omW1sEC3fUsrykmupd9bRUl8aM1hyeFDwZeaTm5pCe4+XoQh9VdiUsU0aCsh5dkObQyEnS8eYlk1qQQkphDt7CnFgQNz4xK1otK2q0luLQcKY4CbSECdpGa9Egrhn0Y4YCmEaoTfAUwHJ6CJlWm4pZkSCuRdCMBHSbggb+kGknYu1ptBYN3uoODU2PJGiZfmOvRmsQCbhaltmh0VqsmpYglpgV/UneUWJWosQbrbXf1hkdVeiKnLdnELcnA5YHq2LWPsxh752II/s99opBX6FQKA4WgsjDyZGKGvQVCoUiHvvX45GKGvQVCoWiHb25uPze6BW/YZzJqdy+9lk+3dnAgrGTKE52ov3xX7z62HNM+MEPuWzLG7z40UZuumAIS46/jX+/+B5Zg4/h1TtOpMmw+N6vzmamZxzT3pyLtEyOPf9kfjwyheW/fYrPKprp63Ey4cbjmL27CZ9T46RJRfS7eSqflpm8P6eE6g2LAcgcMIbxxxZw6cg8/Kakr8fJ8FG59D9/Aow6gyW7mvlwxS52bamhqXwrRqAJzeHim9IG5q2vpHJnA03lWwg21saM1txp2XhzikjP8TKsyMfw/NQ9jNbSHHrMaC21Twre/HRSCnNw5BTuYbQW1fO9eqvRmtfjICktiZDfiCRmxRmtmaHAHkZrUcJoBExJ0Nb1443WWsImAcPEH4q0mJ7fzmhNc2gxo7VoIRVpyVhyVmdGa/G6fkdGay5di+n4Tk2LaPtxhmtdGa1F2ZvRmib23WitKw5Xo7VDzeHW9YjhWmKtN9Lj3RZC6EKIb4UQH9rrA4QQX9sFBd60U5MVCoXi8CA6OUBVztpvfgqsjVt/FPizlHIwUAvcfBD6oFAoFAki0HQtodYb6dFeCyGKgAuA5+11AZwBvGMf8jLQLR7RCoVC0R0I9aR/QDwB/AKw7PUsoE5KadjrXRUUmGoXD1iSlxTggbve5YFnrmbaphpu+eafnP3/3iOj+Ghm3TKcp69/luMyPBz9+jvc9Me5hJob+Nmd59Fv1hNcdXoxjh8/ys+fX0xNyXIGTjqPpy8fTeWTv2HG7G3oAs6aVETf23+G37Q4rSCVUbdfwub00TwxayPbv10aM1orHt2f6yf0Z5BZTqZLZ2x+CgPOG4X7xIvYFHAzY005mzdUU7d9HYH6SoSm48nIY87GKnZsraW+bEcbo7Wk1AySswpJz/HSryCV0UU+hmR69zBay0nSyUl24s31klrkI7VfHkn5+Tjy++E3rb0arSWlJeHJcBP0hwn5/Rj+JsKBJttoLbSH0VqUgCFjRmstYZOmUETL99uaflTPbwmZXRqt6Q771db444uodGW0FtXlEzFa07SODdc60/OBvRqtRTe3n7efCIkarXWHFn8w9fzunr9+uOn5UbqzRu7hRo8N+kKIC4EKKeU3+3O+lPJZKeV4KeX47Kysbu6dQqFQdIwQdJwM2EHrjfTklM1JwBQhxGTADaQBTwLpQgiH/bTfVUEBhUKhOCT01gE9EXrsSV9KeZ+UskhKWUzEK/pzKeU1wGzg+/Zh1wMf9FQfFAqFYl8RJPaU31u/GA5FctYvgTeEEL8DvgVeOAR9UCgUig4RAlzKhuHAkFLOAebYyyXA8ftyftWq9Vx+zDieHnQDv36wljM+aKZqw2Jmv/UQ8089j6qQwU8/fpBzn1/Ojq8/4sTrrueevnW88r3X+eHSN7joX8vYNPdDsgYfwwM3HEvR4n/y9lPzKAsYXFSUxtj7buRLs4gxPjdjbz0JefZU/vKfjaz/ehONuzaTlJpJ0ehxXHvKQE4qTCY0/XmOTkti0DmDyDlnMpXpg5m5qpwvV+6massWWqojRmtJqZmk5A1g5aZqasqqaa7cQbi5HiBWLcuXl01GXgqj+6ZzVLaXglRnzGgtxaGR4dTJSdJJLUghrShSLctbkIsjrx/4cjsI4kYSs3zOSEvyuXBnuHHbgdxYtaxwKGK0Fo4EczsK5AYNi4BptamW5Q+bBOxqWU0Bg5ZQZFt8EFd3RAzWokZrQohYkpaua10arcUHcaPLbZKyHFoseOuMS9DSRGsx62gAOD6IuzfijdbiH+D2x2gtdm4HRmvdHcRN9P7dc73vSBBXgKOXPsUngrJhUCgUijgER7amrwZ9hUKhiEf0Xr0+EY5c4UqhUCj2g8iTvpZQS+h6QpwnhFhvW8/c28H+G4QQlUKIZXa7JW7f9UKIjXa7vjveX6940jcl9P/Pp5x/wb0UPfMrvrzlAX58/93kP3M3f1hZwa9/fSZ/1Y5n4euP0W/ihXww9Xjmn3EmC2v81GxP4ct33sbl9XHFD07lsrQK5t77DxZU+xnjc3PCL8+lcuz3uP+VpTx90RBybriLl1aW88mcEqo2LEZ3ecgdMZFzJ/VnylHZiEX/ZuO78zjqhEL6XnQmxogzmLexlmnflLKrpIKm8q2YIT8OdwopecVk98ujYnsdjWWbCNl6vsOdgjsjj7T8IjLyvBzTP4MReakUp7vJ1MNAWz3fl+slrSiVtH65pPbLQ8/rh5bbDzM1L/YZ6SKSlOXWWo3WPF4X7nQ3SWlJuNM9GP4mzJDffu24cEo8gTaFU1pbc8hoo+f7Q0bMbC1WNCVqtqaLmL6vaQLdITBNC9OwWvX7cKhTPV+a7QzXhMCpter4UaM13Z5b3VnhlPbvLxIraGu01lnhlPY6f0fX64pDUTjlcNfzD3e660lfCKEDTwNnE0lGXSyEmCalXNPu0DellHe2OzcTeAAYD0jgG/vc2gPpk3rSVygUijg00ZoBvreWAMcDm6SUJVLKEPAGcHGCXTkXmCmlrLEH+pnAefv1puJQg75CoVC0IzJDbO8NyI7axdhtartLFQI74tY7s565TAixQgjxjhCi7z6eu0/0CnlHoVAoDhZRG4YEqZJSjj/AW04HXpdSBoUQtxIxojzjAK/ZKb3iST9/5EAm3voihceeyZ13P8moC6/gfzNW8ORj87jmhELq7nicBx9+HW9uX175xalU/vJ63lxcxrl5Xh57Zhb+2nLGXXQ+j547kFW/uI8Za6vIdzs4+9rRpNzw3zz8+WbWfvEtQ+++g3nN6fz94/WULf8SM+Qno/hoxh5fxPXj+5JXsYyd709n49ztDPneRLTjL2Lxrhb+vayU7eurqN++hmBjDZrDRXJ2AelF/Rg4KJOGXSUE6quwjFCkcIovm5S8AWTkpTC8fwajCn0cle2lT4oTR802uxB6RM/PynCTUpBCalEGqf3ycBX2x1lQjJWSTdCVCrTOz3drIjY/Py3ZiTvDjSfDTXK2h6SMVIxAE2F/q9FaR4VToghNJ2hImkMmjcE4PT/OaC2q57eETDSnC93hiFjORufkO0Sb+fqaHrGsjRZOsYxQp4VT2hQ7iRquxc3Lb2+0Fj9PH7o2WouuJ1I4pSMpOxE9PzpmdFY4pSeN1nrDxJPDPUTQjRm5pUDfuPU9rGeklNVSyqC9+jxwbKLn7g+9YtBXKBSKg0U0OSuRlgCLgSF28SgXEUuaaW3vJ/rErU6htf7IJ8A5QogMIUQGcI697YBQ8o5CoVDEIRDdZsMgpTSEEHcSGax14EUp5WohxIPAEinlNOC/hBBTAAOoAW6wz60RQjxE5IsD4EEpZc2B9kkN+gqFQhHHPmr6e0VKOQOY0W7b/XHL9wH3dXLui8CL3dYZ1KCvUCgUbTjSbRh6haa/pjJMsLGGlY9PJq1wKF/dPZr/u/C3HJPu5vjP/sOUB2biry3nV7+4nFHznuLF575hkNfFec/fTuW6hQw5/WL+cf2xVD16Fx9M34gpJZNP7ceAX/6G51bWMGPGampKlrOlcBIPz1hHydeLCdRXklY0lEHjh/HjkwdyFOWUv/saGz9cz/L6AN4zLmOTkcY7y8tYuaqCmi1r8NeWx6plpfcdSsGADE4fnktLdVmbalnenH5k5qXQvyiNcf3SGZGTQkGKA2fNNsyd6/HZSVk5yU7SilLx9UsnrbgP7r59cRYUY6blY6bkUBsw22w7i/8AACAASURBVFTLiiZlxVfLcme4SUpPISk9hXAgkpwVNVrrKogrNL1NtaxY1ax4ozW7apY/WjlLjzda6yRJy6ElVC0LiO3vqFpWNEHLGd0eVzkrkSDuHu+5i2pZmqAL27XOSSSIu79ji6qW1YOoIioKhULx3SHqp3+kogZ9hUKhaIca9BUKheI7gqaKqBx6Ag11zH3+LuYMm8D8JfP5z8iJ+E2La778BxMfW0jp4hlc9JNb+a/kdTx95+uYUnLNb87l26OvIn9MGk/cOoG8mU/yypNfUBYwuGxYFuMeuouPm3L569uLKF85D6fXx8OfbWTtglU07tqMJyOf4mPGMfXMIZySK2h+52XWv7uUb3Y1URk0KU0dxLTlu1iwrIzyjetprtyBtEzcvhzSio4iv38GZ4zMY2JRBuHmelvPz8Sb04/0PjnkFaYxfkAmR+em0jfNibelAnZtIrR1HdkunXy3I6LnF6WRNqAP3n6FOPsUIzMKsFJzqQ1a1AXMPQqnZLr0WNGUSPPizvLhyfJhBv1YRrjLwilRPT+q6bfR9QMRo7XGgEFT0KAxEMYfMgmFzJhe73DqrQZrcYVTYlq/Jjo0WevIaC267HJoODWt08Ipetzyvuj5XRVOidfzu7pGIqjCKa0c9no+xDT9I5VeMegrFArFwUIQ89U5IlGDvkKhULTjSLaSVoO+QqFQxCEgNv33SKRXDPpFffPR7riCT3c2kH7FhXxW0cyj7/4XV3/p5Nv3X2fcpVfzxuRspo+9iQ1NQX5y01iCtzzC1N/P4dc/PoVTKmbz0d2vs7w+wFm5Xk569HpWF5zCb59fxNZFnyM0nX7Hnc7sz9ZRvWkpTq+PorET+cFZg7l0WBbmp39j3RsLWLq+mh3+MB5d8PHGaqZ/vYNdG7bRtHsrlhHC6fWRVjiU/OIcThyRy0nFmRyVlQRECqEnZxWQ3qcPuUVpHDcgk9H5aQxITyLdakSr2EywZDW1a7dF9PzCVNKLfaQNyCetuA+OggGI7CKM1DzqDY3agMmuxiApDi2uELqOJz0pZrKWnNWq57sy0jFD1V0WTonX84Wm0xgyaQoaET0/aOAPmTQGjJjRmj9kEgyZWKYV0fLjjdWiGn7cnH2H7UHeRse3+9OZng+0MVfThMCpizaFU+KX94X2en5H5msQGQQ0IQ64cEp7Pb+75+gf7np+r8H+WztS6RWDvkKhUBwsBOBMsBRib0QN+gqFQhGHkncUCoXiu4Q9JfhIRQ36CoVCEUc0hnOk0iuEq/T6XTz34UYeeOZqXp69jft+N5nHUy/gg6dfZOApFzPnFyfzxTlX80l5M9edOYB+T73BFc8sZOPsD/hRdjlzb3mUT8qbOSbdzVkPXUzFpJv46RvL2DB3Doa/ifwxp3PthcOoWL0AzeGiz+iTmXLmIK4dnY9z4dusf/U/LF28i83NIXQBg7wu3vhqGzvWlVK3Yy1GoAmHO4W0PoPIG1jIMSNyOX1INkfnJuOpWI/DnYInqwBfQX+yC1M5dkAmx/ZNZ2iWhxxHCEflZkKbVlC3fgt1m3eRmeclvX8avuI8fIMKcRYNRs8fgOnrQ7NwUxs0KWsMUtoYsIO4OpkunZQUF+70SBDXEw3i5qSTlOlD82Vh2tWyosHTjogGcXWni6ZQJIjbHGpNyoqvlhUMmRhhEyNs7WmsFk3IckSqZTniikm3T8zqKIgbRVpmnLmaFquSFZ+o1Vo5izbnxdOhsVy7ClnRIG6b4O5+/s1G6ew/WPcHXbv7et0/6PWmcTRi7Lf31htRT/oKhUIRh7AfKI5U1KCvUCgUcRzp8o4a9BUKhaIdvVW6SYRe8Rtm1+5GfnnPyTw96Abuunkscyf/it8/8FdyR07iswfOZOUlk3lrZQVXjMpl3Jv/4qLnFrN8+vuk5BXz1fX38O/11QxNcXHRL87EvPq/ufPdlaycOQ9/7W5yR0xiyvlHcceEIqRlkjtyEmefMYjbTuhHxrqZbH7pbZbN2c7axkix+kFeF2NGZLNl1S7qtq4i3FyP7vKQkl9M7qCBjBqeyznDchnXJwVf/RZCqxaQnF1AWp9icop8jB2Yxfj+GYzI8VLgtnBWbiK0aQUNG0qo27CT2pI6Mgam4xuQi29wIa6igTgKBmL68mlxpFDtN9ndGGJXY5CdtX5SHBqZLo2UFFckISs7GU92Mp4sH57cdNxZET1f92V1qefHJ2XpThdC02kKGjTbRmtRk7WmQDii7YdMTNPCCFsYIRPNoeFwtjVdczi1WGGVqJ6f1D45qxM9Pz45K17Pd+hanIbfquc79Va/lEQLp0Br4ZSu9HxNiP3So7u7cEqn9+kFA1RvenAWtBr47a0ldD0hzhNCrBdCbBJC3NvB/p8JIdYIIVYIIWYJIfrH7TOFEMvsNq39ufuDetJXKBSKeLrRZVMIoQNPA2cDO4HFQohpUso1cYd9C4yXUrYIIX4M/C9wpb3PL6Uc2y2dsekVT/oKhUJxsIho+om1BDge2CSlLJFShoA3gIvjD5BSzpZSttirC4Gibnw7e6AGfYVCoYgjasOQSAOyhRBL4trUdpcrBHbEre+0t3XGzcDHcetu+7oLhRCXdMf76xXyTm6Gm/k/eITf/fgRTnrteW679QlS84r5+JGLqfvJlbz4SQkX9/dxyox/cM20nSx86z2SUjO44qYLefPK5ylwO/ne7RPx3fUYt767iq+mz6WpfCvZQ4/jnAvGcN8ZA0n6/Hmyhx7HKWcM5a5TBlC062tKXvonyz7ezPL6ACFLUpzsZNzgDAZPGUvtzOUE6ivRHC5bzx/KsGHZnDcyj+MKUsluKcNYtYCqhUvxFU4mp8jHqEGZTByYyZj8FIq8Go7yjYQ2LqNhzTpq1m6jemMNDTsaKTqxmIyhfXH3H4Sz31AMXwH+pAyqWgx2N4UobQiwvbaFbdUtDHbppCU7cWe4Sc7ykJzticzPt/V83ZeFnpGLnpGz10LomsOF0KPLTprDJvUt4UjxFFvPjxZCN8ImRsjCNCxM04qYqsXNz9d0EdPzPS49pue7HPoexVOien6U+H5Ky+pQz3fGLUeKoosOTdE60/OlZbYphA6d6/n7Q6J6/gHnAfSAVn4kz1xJCAH7MGOzSko5vltuK8S1wHjg1LjN/aWUpUKIgcDnQoiVUsrNB3KfHnvSF0K4hRCLhBDLhRCrhRC/tbcPEEJ8bQc13hRCuHqqDwqFQrGvRKdsdlMgtxToG7deZG9re08hzgJ+DUyRUgaj26WUpfZrCTAHGLffb8ymJ+WdIHCGlHIMMBY4TwhxAvAo8Gcp5WCglsjPGYVCoThMELad995bAiwGhtgPuy7gKqDNLBwhxDjg70QG/Iq47RlCiCR7ORuYBMQHgPeLHhv0ZYQme9VpNwmcAbxjb38Z6BadSqFQKLqD7nzSl1IawJ3AJ8Ba4C0p5WohxINCiCn2YX8EUoC3203NHA4sEUIsB2YDf2g362e/6FFN356u9A0wmMi0pc1Anf1BQBdBDTsgMhWgT7K7J7upUCgUMSI2DN0X15BSzgBmtNt2f9zyWZ2c9yUwqts6YtOjs3eklKY9x7SIyNSlYftw7rNSyvFSyvHeAUO57b8eo/DYM7n0jmdweFL49+M/xPXgTfz19dWcm+fl3M9f4Nb5IWa8+Ba6w8VFN1zCk2fmkenSufKmcfT5zf9xz0fr+eTdeTTs3EDmwDGcdsF4HjhnCOlf/Yul//s2E848mnvPHMrgmuVsfe45lr+3lqV1AfxmJIh7/KAMhl4yltyLvoe/dndcEHcYR43I4eIxBZzY10deqBxj5TyqvlpM2dclkSDu4CxOHJjF6LxU+qW5cFZsJLzxWxrXrqVm3TZqNtZSv62BMn+YjKH9cBdHgrimr4BgchbVfoOK5hA76v1sr/OzrbqFnTUtZHideLKTSclNxpvnxZObQXJOOp7cDBwZOWgZuWi+LKQnDcsI7fE5tw/i6g4XmsOJ5nDFgriNQSNmstYUMGJB3IjZmolptFbOcrh0NF3EErTik7JcDr1N5SyzXaJY+4pe0rKQlhmrmtVZENcZrZ7V7q+5qyAutAZxoxW0Yp+J/Rp9kjuQuOahDOLuz/W/80FcGyESa72RgzJ7R0pZJ4SYDUwE0oUQDvtpv8OghkKhUBxKtAP+Sj586cnZOzlCiHR72UMkI20tEW3q+/Zh1wMf9FQfFAqFYl8RqCf9/aUP8LKt62tEAhgfCiHWAG8IIX5HJP34hR7sg0KhUOwzvcHPaH/psUFfSrmCDuaU2vNNj9+Xa5Vs3U2/H0xi5eOTKfreWt594hZyHr+dx/++hNNzvFw8/wXuWOrk7b+9htB0zr/hMl64qJgNt1/PNTeMpe8jz3LPJ9t4//U51G1dRXrx0Zx60UQeuWA4uUveZOkj/2TWN7v4n7eHMbx5DVv//le+fWMlC2v8NBkWfT1OxhenM/TSMeRdchn1/SeiOd4mJb+YnMEjGDIih0vGFjKpXzp9wpVYq+ZRtWAhZV9vpnxVJSNvzuLkwdkcW5DGgHQXrvL1GJu+pWndGqpXb6F6XTW1JXWU+cPsDhh4Bg3BVTwMM6MvQW9OLClre32A7XV+Siqb2VbVTENdAE92MsnZnk71fD0jF7zpWG7fHp9rV3q+5nTF9PyoyVpner4RtnAlOTrU8z0uvY2e79K1NkZrQMxorSM9H6KGa2Kven68Hr03PT9KvJ6vic71/P35Saz0/F5KL36KT4SEB30hxIlAcfw5UspXeqBPCoVCccgQJDwHv1eS0KAvhHgVGAQsA6KPShJQg75CoTjiUPJOxA9ihJRS9mRnFAqF4nDgCB7zEx70VwH5wK4e7ItCoVAcclS5xAjZwBohxCIinjoASCmndH5K9+HwpLD2yQuYPWwC0+d8Tu6ffszjzyzi9Bwv3/vyH9z2jYs3nnkNgAtv+j4vTenP+tuu4/X31vNA1TfcZQdxa0qWkzlwDKdeNJE/ThkRCeI+/DKfLSqjLGAwsnEVW55+ao8g7oQB6Qy/4hjyL7uC+v4Tmb21LhbEHTEqj0vGFnJK/3QKjEqslXOo/OJLSr/cRPmqStY3hjhtaE7bIO7GpTSsXNFhELfBsGJB3IA3h0o7iLu11r9HELelIbhHUlZyn6wOg7iWO63NZ9pVEFdP8qA7XAkHcS3DSjiI63JobZKy9hbElZaZcBC3s8pZUVQQV5EoR/CYn/Cg/z892QmFQqE4nDiSC40kNOhLKecKIfKA4+xNi+Ld4BQKheJIQXRjucTDkYS+0IQQVwCLgMuBK4CvhRDf7/oshUKh6J2ojNyIuf9x0ad7IUQO8BmtFsk9ytFFqXw8YDzzqlqY+tsb+dNLK7iwTyrnf/VPrv8izHt/exmHy8OVt13B06dnsuqGa3ljxiZMCbdOL+GjN2dRv30tWYOP4dxLT+Th848ic8FLLH74NWYtK2d3wKA42cnmP/+Zpe+uYWGNP2ayNmFoJkdddgx537uSmqLjmVVSyxuLd5A3ZCRHj87j0nGFnNTXR35wF8by2VTO/5qdX25i95oqNjWFKQ8aXFSYRnGaE+fudYQ3fEPj6lVUrdhM1bpq6rc1sKMlTGUwouf7TQsjsz/B5CwqWwxKGyIma1tqIpWy4vX8lsZgTM/35mfGkrL0rD7oGTlYyenIpFQsj4+Q1lqrJhE9X3O4utTzI5q+xLIrZyWq5ydFDdfMVs2+Kz0f9m6yFtXzO6qcFaXDimFdVMpqr+eL/fwfvjc9v7sfKHvpOHRYIVDyDoDWTs6p5sj+XBQKxXeY/f2S7w0kOuj/RwjxCfC6vX4l7fyhFQqF4ohAqOQspJQ/F0JcRqRcF8CzUsr3e65bCoVCcWgQQDfWUDnsSNh7R0r5LvBuD/alU2pWrmeh1ocHnrmae299je+PyOHUOe8y+e0dzHv5NTwZedz6k+/z0BjJ4it/yFvztuPRNa6aPIgzX/mQpvKt5I6YxCXfO44HzxmM+z9/YeHv3+XztVVUBk0GeV2cMrGQBW+tZmldAFNKhqa4GD8im2GXH0f2pddQnjWSTzdW89rX29m6tpLjJxRxiV00Jad5O+FvP6f8i0WULdxC2bpqNjWFKA8a+E3JgBSBc9dqQusWU79qDdWrtlK1vpq67Q2U+g3KgwZNtp5vSmhxZ1LVbFDaEGR7fYAt1c0xPb+pLkBzQxB/U5BgYwPeoiw8uekk52eh23Pzo3q+5fYh3akEhIuWkAW01fN1p8tedqK7PGhOF7rDFVl2uKhrCeMPRfT7cDA6L79VzzfCZkzTj+r5HpfepmiKx6Xj0qPrkbYver5lmW30fKfeqt+31/PbF1GJ0pnO35Ge311afvz1oyg9v/dwJMs7XeryQoj59mujEKIhrjUKIRoOThcVCoXi4BHJyE2sJXQ9Ic4TQqwXQmwSQtzbwf4kIcSb9v6vhRDFcfvus7evF0Kc2x3vr8snfSnlSfZranfcTKFQKHoD3fWcb9cTeZpIEamdwGIhxLR2Bc5vBmqllIOFEFcBjwJXCiFGAFcBI4EC4DMhxFApZcc/XRMk0Xn6ryayTaFQKHo/EbkwkZYAxwObpJQlUsoQ8AZwcbtjLgZetpffAc4UEX3pYuANKWVQSrkF2MQ+1iLpiESnXY6MXxFCOIBjD/TmCoVCcdiRYGKWPeZnCyGWxLWp7a5WCOyIW99pb+vwGLt2eD2QleC5+0yX8o4Q4j7gV4AnTsMXQAh49kBvnighS/LbD+/lMedp3HjWV4ye/jEn/2k+377/JpkDx3D/PRcyNW0rcyf/gndXVVDgdnLZD0Yy6OE/0zz59xQeN5kbLx/FL07qR+DVh5j36Md8tr2eJsPi6LQkJp3en+G3fZ9XJv8egOGpSRx7bD7Dr5pE6vlXs81TzH/WVfLWwu1sX1dJTckKfnDHRI4vTCW9egPBbz5j17wllC7czs6SOjY1hagKmYQsiS7AWbqC4LpvqF2xlurV26jeWEvVjkgQtzZsUh828ZutrtXlLZEg7tY6P9uqWyipbKKsxh8L4gZaQgQbGwi31JPcJ5PkvGzbYC0HPSMXy+PD8viQSan4pU5zyMJvWK0JWZqO7owkYMVXynLYAdxoklZLNCkrbGHYAV3TtDBCkeBtNIhrGhYuO4Drcmgku/Q2SVnxQVyXnZwF8YFcK7Ye/2rZr4kGcds/eXUWwI0n0SDuvgZdVRC39yKkRCTwt2NTJaUc35P96W66fNKXUj5i6/l/lFKm2S1VSpklpbzvIPVRoVAoDipCWgm1BCgF+satF9nbOjzGVlF8RBJgEzl3n9nb7J1h9uLbQohj2rcDvblCoVAcfkiQVmJt7ywGhgghBgghXEQCs9PaHTMNuN5e/j7wuV2wahpwlT27ZwAwhIgH2gGxt3n6PwOmAo91sE8CZxxoBxQKheKwo5uKBEopDSHEncAngA68KKVcLYR4EFgipZwGvAC8KoTYBNQQ+WLAPu4tYA1gAHcc6Mwd2PuUzan26+kHeqMDoc/IAVxTNoYZf3+SKz+cxjH3fcrmOf+m8LjJ/O2eUzh1y7+ZdsmTfFLezNFpSVz6s9PI+vnj3D97G4NPncIvrhnLD/pJKv73Lr56Zj7zqloAmJTl4bhLjmLQj26gbvg5uLRHGONzM+bkvhx19ek4T7uK9TKL95fvYsbinexYV0r99rUE6is5tb8P945vaF44k9J5yyhbVMaWnQ3s8BvUxOn5PqdO4Nt5VK/cRPXanVStq6G6sjlOz7cIWZE/MF2ASxOU1PjZXh9ga1UzJZVNlNf6aW4I0lIfpKUpSLi5nlBLPYa/ieQ+eehZ+egZuWi+7Iie707FcvtoMSQtYQu/IWkOm53q+fEma3pSRNd3uJx76PlGOKLft9fzLSMUp+VrXer5e2r6Xev50owkZ2mCver58ZJ+onr+3gzWDlR77+h0pecf5kiZ6FN8gpeTM2hnWyOlvD9uOUDEwbijcx8GHu62zpD4lM3LhRCp9vJ/CyHeE0KM686OKBQKxeFCN2r6hx2JTtn8jZSyUQhxEnAWkZ8jf+u5bikUCsWhQoJlJNZ6IYkO+tHfyhcQMVv7CHB1cbxCoVD0TiTdGcg97EjUcK1UCPF3IqnEjwohkjiIfvprq01WPPV3+k28kFPu/CfVm5Yy6sIrePunk0h99X6e/58ZrGoIcm6el3P+fDW1597FVa+t5MuPvuTtP13HyZSw4b7fM/uddSyvD+Bzapyc7WXMTcdTeOOtlKQN56UF2zglO5kRU46i+IqLYMKlLK42efPbbcxfWsquDVto3LWZYGMNQtNxrZlFzYLPKZ2/hl3f7GZTVQtlAYP6sIkpI9q8z6lR4Haye+FKqtbuprakjl3V/lgB9CajrZ7v0TVSHBobqpspqWhmW3Uz1bV+WhqCkfn5zQFCjTWEA00Y/ibMUABH/lD0jBxIycL0RAzWzKQUmuy5+S1hi+aQRX0wHDNUi5+bH9HvPW31fNs8LRS0tfyQiWVKjJDZquObFpYlsYwQVjgU0/M9LkebgilRHV/XRGwZEtfzgTZ6fvu5+pH9ET1fo+vC6O3ZFz1/f/y3enpufkf3UHQHEqzeOaAnQqID9xVEos/nSinrgEzg5z3WK4VCoTiEHMmafqJ++i1CiM3AubbT2xdSyk97tmsKhUJxiOilA3oiJDp756fAv4Bcu/1TCPGTnuyYQqFQHBKkBMtMrPVCEtX0bwYmSCmbAYQQjwJfAU/1VMcUCoXiUNFbpZtESHTQF7TO4MFePmgxJH99LZPu/iGf/mQieec9wCU/+RGvXjqQDbdfxfPvrMNvWlxzQiET//Irvso4gf/3l69Y+/lnBOorOWHLdL7+/UvM/KqUsoBBgdvBaaNzGTP1DJKnTGVBo5dnP1nPoq93ctutJ5L/vctpGnQSs7bU8driHaxbVUHF5nU07d6KGfKjOVwkZxVQ/uE0Sr/ayO7lFaxvDMWqXwF4dEG2y0Ghx0GfLA+7Fu+gtqSOMn84FsSNVsmCSNDXowu8uobPqbO6tIFtVc001AVoaQjS0hgk2NxEuLm+TRDXCPrRcwrBG6mSZbnTCOlJNAdN/GGLlrCkMWRQHzBoChl7BHFjAVy7apbDlYSmazhcOg6nTjho2NWy2iVjmRamYWAZIaRpYhmhSADXTshqH8SNNV1DEyIWxO0sgAutQVwAp6Z1mZAVDeAKkVgQN/6YvQVx97eAUiJB3AOpzqQCuD1J9yZnHW4kOuj/A/haCBGti3sJkbn6CoVCceTxXR/0pZSPCyHmACfZm26UUn7bY71SKBSKQ0U32zAcbuzNT98N3AYMBlYCz9gm/wqFQnFEIvhua/ovA2HgC+B8YDhwV093qj0FRfnMPifMzGETePq9j7jKXMrnx93G+xtrGOR1cfNtx9H3gcf588oW/vbcHEq/mYnmcDHgpCl8duP9zN7dhN+0OCbdzcTzBjL0R1cROuFyXltbxYtzVrJ56WZqt60i/82fsdVVxEfLd/Pewu1sX19Fbcky/LXlSMvE6fXhzelLZr9BbJz+MTu21rGlOdymYEqKQyMvKaLn5xamkjkkk41zt1PqN6gKRXT/+IIpHl3g0TXSHBqZLp1Ml86/yxpoqg/gbwzhbwoSbKyLGayZoQBGyI8VDkU09TS7aEpSKgFL0By0bJM1i/qAQX0wouc3BQ10l3tPPT/OYE3XNRxOHc2h4XBqdmJWxwZr0jKxwqFYIRSPS+/UYE3XBC5dw6kJNE3sk54vLXOven5Ml09A6G6v53dVMCVecj+QTMQjTc8/gK73EiSYvXNmTiLsbdAfIaUcBSCEeIF98HIWQvQFXgHyiCQ2PyulfFIIkQm8CRQDW4ErpJS1+951hUKh6AGiNgxHKHt7gAlHF/ZD1jGAe6SUI4ATgDvs6u73ArOklEOAWfa6QqFQHDZ8lzNyx7SrjRutlSsAKaVM6+xEKeUuYJe93CiEWEukqO/FwGn2YS8Dc4Bf7u8bUCgUiu7lOxzIlVLq3XETIUQxMA74GsizvxAAdhORfzo6ZyqRql0U+lJ4dOIdVIUM/t/0h3jij7PZ3BzioqI0znjqRkon3cLk15az7JP5NOzcQGqfQYw4/UT+e8pI3v9LA5kunbP6ZTD6xhPIu/ZWNnoG8uzMzcxcsI2yVctoKt+KtEzm+bN5+6sSFn5bxu4Nm2nYtZlwcz1C00nOKiC1z2Byi/MZPCiTVS/VsMMfpsmwYgZrmS6dvCQH/VJdZAxMJ+uobDKG9uXjjzZRGzZjx0Jbg7Wonp+d5CA520NdZTP+plDMYC3UUo8Z9GMEmjGjWr6tpZspOZjOZJrDrVp+Y9CMm59vUh8M0xQw4vT7jg3WHE4dh0uLaftNdYE95ubHa/nx/fA49T30/KjJmlPTIgXibV0/ek6U9gZr0FZ7d2raHsXP4/V8TSSmM7efw5+IwdrhpOXv+/27915HvpYfxxE86Pe4U6YQIgV4F7hLStkQv8+uA9lhXTIp5bNSyvFSyvFZXk9Pd1OhUCgiHOE2DD066AshnEQG/H9JKd+zN5cLIfrY+/sAFT3ZB4VCodg3JNIIJ9QOBCFEphBiphBio/2a0cExY4UQXwkhVgshVgghrozb95IQYosQYpndxiZy3x4b9EXkd+wLwFop5eNxu+Irv18PfNBTfVAoFIp9RnKwnvQTmdTSAlwnpRwJnAc8IYRIj9v/cynlWLstS+Smidow7A+TgB8CK4UQ0c78CvgD8JYQ4mZgGxGvfoVCoTgskMg28aUeZK+TWqSUG+KWy4QQFUAOULe/N+2xQV9KOZ/O80jO3JdrlZXVk5OeyR1PXM6vb3uNAreTu24ey6CH/8zzWwRPPDiL7YtmAtBv4oVcfsEwfnpSfzK+eZeNaUlMOr0/w2/7PvK063h7fTV///cyNi/bRvWmpYSb63G4U/AVDeWhD9eyfV0lNSUraKkuQ1omDncK3ty+ZPYbi5IBiwAAH8RJREFUQn5xOhOG5nDSoCw+awrFErJ8Ti1msJbfJ4XMIRlkDi0gY3h/kgYMozw4rU1ClksTsQCuz6mT6dLwpSbhzU0mJc9LY62fYGMD4ZZ6Qs31eyZkxT1h+DU3zQETvxEJ4tb5I8lY9cFIQlZD0KC+JUxjwMDpTokkZ9lB3I4SsqIBXd0RSc6KBnKjgdv4hCzLCGHZy9HKWZ0lZEWDuQ5dSyghK569JWTFm651RGcmbPuSkLWvAdhDGcTt7gAufNeCuOxL5axs8f/bO/cwueoyz3/ec6qqu7o76a6+pskdcoWgEUMQYUUYVHRcQAEF3R3dgWHcGS8MuoLyeBlHn0V3V2ZGXUccFEd58O7IiBoBEdYLaIAkhJCQkAu5kk7S3elbXU6d3/5xflV9qrsqXZ1LV1fq/TzPearO71x/6cpbp77vTWRtaP1uY8zdZR5bVlBLDhFZTdCm9sXQ8OdE5JPYXwrGmNREFz2VT/qKoihViJmMdHPIGLOq1EYReRiYVWTTHQVXNMaISNGgFnuebuDbwHuMyYcWfYzgyyIG3E3wK+EzE92wGn1FUZQwxpywk3b0VObyUttE5GUR6TbG7D9WUIuIzAQeBO4wxjwROnfuV0JKRL4JfKSce5qy5uaKoijVgclLmBMtJ8iEQS0iEgN+AvybMeaHY7bloiCFoNz9xnIuWhVP+h0t9fy35x/kC89luXrpGl73pffzwjlv5/LvPMOGNY8y1LOb5nnLWXHpBXzu6nO4wN3H/i/ewsPfeJJ3fvotJN5xM5vkDL76sy08/vuX2L/pGYZ6dgPQ1LWAtrPO5cxzOln/yFqO7nsRLzmYb5Yyc85SOua2sXxxG5cs6WD1nBYWtsT4uW+Iu0Ii6jKrPsLc5jpaF7fSuqiNxPL5NC1aRHTBcvy2+Xk9P5eQFdbyE7EIjV0NNLQ30NjZQGN3K8O7Xy4osDY2IStMf8pnKOMzlM7Sn/LoT2YYTNvkrOEgKatvJMNgMoMbixckZEVibqDphxKywtq+l8keMyHLD3344yFN37UaftQNiqSN6vqS15snSsgKr0fdkIZfJCErrPGPZaL/mCdbyy/Gsc5RTpG4yaAJWSeBXPTOqadoUIuIrALeZ4y5yY69DmgTkffa495rI3XuE5EOAt/pOoKKyBNSFUZfURRl6jCTceQe/1WMOUyRoBZjzFrgJvv+O8B3Shx/2fFcV42+oihKGMNUhWxWBDX6iqIoBUwqeqfqqAqj781ZyHlf3My2x37Owad+yy0/f4EHbv0ePZufoL65g3Pech0feNs5vHtZM5mf/hO///IafrfhIDuHM6x872e5a8N+fvjYH9m1fhP9e14gmx6hvrmDlgUrmLtsNpe/6gzeuryLi74etP2tb+4ItP758zhjQQuXLu/kwnkJlrbHaUsfRrZsGi2u1hChbX4z7UvbaFkyh5alC4kuWIZ0n4XXMofebARXwrH5gZbfGnNpStTT2NlIY1cjjZ0ziHckaOxuY2TDgXzj81Javjgu4rj0Jr3RZikpj4F0lqPJTD42fzDlMZi0cfqNzaNx+PkG6I7V8cfo+xEHL50aF5c/Vss32UJNP6fhR20T9Kgb6PhRR3Ctpu/b48KU0vNhfHG1sWMwXhsvx8k2Vs8/lpZ/vNp7KT1ftfxpzEmM3pmOVIXRVxRFmTr0SV9RFKV2mLronYqgRl9RFCWEwWCmIHqnUqjRVxRFCaNP+pXnxZ0HiD76H8w5/42svPXn7H3qIZxIjIUXX8m7r1zO+18zl4bffYeNN/yAPzz+Es8PpHBFOK+lnnd9409sX7eTIzvWkxnqJ9rYTGLBCs5YdiYXrTyD/7xiFqu6G5l5cBPRxmYaO+bSOu8sZi1o4eJlnbxmQSuv6GpklpvE3f80qeeepHfjVl7ZXEfn7BlBQpYtrhZbsAx39hKyiTkcdRroGfLYc3SY5qib747VGnOZ0VxHQ1ucpq5GGjqbaOxuI97RQryjFbdtFunhDUWLq+UQx8WJxBDHZc/RFIPp8cXV+m1C1nA6y3DSw8tkidVFChKw8slZURc3IjiuQyyUZJVNjRQtrhZ24AL5zlnFiqvlOmY5Ivn35SRkhXFDna3CxdUKHLsEzszJZEmWk5BVaw5cqHEnLgSO3Ey60ndxyqgKo68oijJ1TE1yVqVQo68oijIWlXcURVFqBGNORjG1aUtVGP1IfSO3ffYWbr9kAa2XfJjZr34D1/z5Mm593QISz/w7m2+8hScf2sH6/iQAy2fUcd7KLpZf/1r+7r6f5hultC06j1lLzuL8V3Zz5bndXDhnBi1HtpJa8xA7Hl/LnFddX9Ao5dzORs6IZYgeWE9q81Mc2vA8h5/bxeGtvSz9T3MLGqW4cwItv99tomfEY+/RIXb2jbDr8DBz49FxjVIaOpto6EwQ72yhoasdJ9GJm+jATXTijfx+Qi3fjQbNUA4MJAu0/HAy1kg6Sybl4WV8vHSWWDw6rlFKJOqM0/LrIg7xWCTQ8SfQ8oPFpy7iHFPLzyVq5fT5crT83NhEWn6wT+mia8eiXC3/RGVu1fKrC43eURRFqRWMwWTV6CuKotQExhj8jFfp2zhlqNFXFEUJY9An/UqzYu5MPrj1Hn7z17/io1/7Hh987Twa/3A/z/2XW/hhKC5/xcx6zju/m2XXX0Tjm27gpfp5mH+7g/Yl59O9ZCEX2rj8889oovnQZlK/fJgdj61l35/2sOvFXt7+rU9y0ZmBlt8dSeLuX0d681r2r9/Mkc27ObT5MIf3DbJ3xOMvP/gm6s46Ox+X3+c00DPssefoEC/1j7C9Z4hdh4fYc2iY2zsbCuLyGzoTNMxqzcflu4lOnJYO/HgzXv2MosXVxmr5TiSKE4mxu3ckH5c/kvYYSHr5uPyclp9rcF7fGD1mXH7Q3Nyuuw5eemRCLT+3Xu86JePyHQli7XMNzsPzO5aWnyPnB5hIy59sG7jc/pXU8o/n/KdCz1cKUaOvKIpSIxhj8LWevqIoSu1wOkfvaGN0RVGUMDZ6p5zlRBCRVhF5SES22tdEif2yIrLOLg+ExheKyJMisk1EvmebqE+IGn1FUZQQueidcpYT5HbgEWPMYuARu16MEWPMSrtcGRr/PHCXMWYR0AvcWM5Fq0LeObJhM5/8YC8xR/iHQ9/nmTeOdsZqijhc1NbAuZcvYNE734B78XVsTs/g++v38fBTT/Kqq67Od8Y6t6Oe6I4nGfz+w2x5bD37njrA9n0D7B7JcCSd5RMXzw86Y730DKnNa9m3biuHN+3jyLYj9BwaYe+IR28my6DnE7/sOrItc+jJRugZ9tjVN8Du/hF2WAfugcPDDB1NMXQ0xayVXQWdsWJtrbiJTty2WcjMdkz9DLx4M37dDIY9A4x2xhLHxYnGcKwzN+fAdeviuJEYe3pH8slYqXSWdC4ZK5PF93y8jE/W88lmfZqa4wWdseIxlzrrxA07cHNjvpfOO3ALnbijDtzcazzq5jtjjXbJKnTgBs7d4slZpcageGG13DgUd8iWQykHbrGzTDa56lQ4cJWpw58aR+5VwOvt+28BvwFuK+dACT68lwHvCh3/aeCrEx2rT/qKoihhbMhmmfJOu4isDS03T+JKXcaY/fb9AaCrxH719txPiMjVdqwN6DPG5H5u7AFml3PRqnjSVxRFmTIml5F7yBizqtRGEXkYmFVk0x2FlzRGREyJ08w3xuwVkTOBX4vIs0B/uTc4FjX6iqIoIQwnL3rHGHN5qW0i8rKIdBtj9otIN3CwxDn22tftIvIb4FXAj4AWEYnYp/05wN5y7qkqjH7aN1x3Xjcrb76UT7/3XgY9n7nxKNcsa2PZNSvpvuY6hpZcwi939PHdn+1m44aXOfjiZgYP7GTTg3cyxz+Mv/E/OPTN37H3D1s5sP4g2wbT7Et6DHrBHzfuComnfkT/xvUc3riDw1sO0bu9j72DaXpSWXozWUayPln7Xby7fh4vH86ws2+QnUeG2d4zxJ4jw/T3JRk6mmRkIM3IwACZoX66L1hCvDNBtL3LFlbrhMYW/PpmsvUzybh1DGd8hoY8hjPGavcxxHVxQzq+E40RicUDTT8Wx4nG2HNkmFTKw0v7NiErS9Zq+b7V8rOej2+Ts2IFWv6ojp8rtBYLLX4mPUbP9wt0fADfvtZHbEKW1fKjjlOg44d1/XKKrYVxc9r9BFr+ieruYw8/2UXSVMevEozBT09JGYYHgPcAd9rXn47dwUb0DBtjUiLSDlwEfMH+MngUuBb4bqnji6GavqIoShgDvu+XtZwgdwJvEJGtwOV2HRFZJSL/avdZDqwVkfXAo8CdxphNdtttwK0iso1A47+nnItWxZO+oijKVGGYmiqbxpjDwJ8VGV8L3GTf/x44t8Tx24HVk72uGn1FUZQwhoI+zqcbVWH0u8+ez8I1D/GVdfs4P/HvnHvFWZx5/VtwXnsNzw7E+NK6vTz6wP9j/wu76d/9PKmBI4jjUjejlZYffI4tv32W/U8fYNv+IfYlg5j8rIGYI3TUuXTVRZjXEOXZ//0tenf0caBnmAPJbD4mP+0HQr4r0BRxiLvCg1sPsf1gEJN/qHeEwb4kw4NpkkNp0gNHSA/3440Mkk0naV61GjfRgcxsx483k403k61rCnT8jM/ISIbBlE9/KsNgOks03pSPyXfrrIYf0vHdWDzfCOVoX3JcTH5O1/ezPr5vgkYomTRtTXPzMfnxqFug47uOFOj5UcfG6ReJyYdRLT/3n6Mu4hSNyQ+vhxuhlNuZyPjZokXViun4x1OHrNyY/MnmAEx0DWU6Y7QMw/EgIt8QkYMisjE0VlbasaIoSsWYXJx+1XEqHbn3AleMGSs37VhRFKUiGGPIpr2ylmrklBl9Y8zjwJExw1cRpAtjX69GURRlWmFsePLESzUy1Zp+uWnH2HTmmwHmdZfcTVEU5eSinbNODROkHWOMuRu4G6Bx9hJzwc330L/nBfo3/YrN6Rn8z2f3s+bLG9i7ZQ/9u58n2d8DQH1zB+1Lzqd17lxmn5ngBx+/MV9QLWsCZ2xzNOe8jdA2v5n2pW20LJnDfXc+UtR5G3eFpojDzIhLa8yhNebyj7/flS+oVsx566VGrCM0i7t0NX6ooNpwxmfoaJqRjE9/0qM/5TGY8hhIZzmazBBrSuQLqhVz3rquQyTmEok6DB9N5QuqFSRj2WvnEqx8L01rU11BQbWxi2uLpeU6X/leJvhblHDe5v9WfnY0OauE8zZcNG0iJ+74zlnB67Gct8fzkzXsYD2dnLfaWOsEMWCyJU1T1TPVRr+stGNFUZRKYTBTVWWzIkx1Rm4u7RgmkTasKIoyZRgwvilrqUZO2ZO+iNxPUCu6XUT2AJ8iSDP+vojcCOwC3nGqrq8oinI8GAPZtCZnTRpjzA0lNo1LO56Ikb5eYgNHmP3qP2P1FzdwYMtWBl7eSWaoH3Fc4okuZr3yUjrmdrB0cRuXLO3kgjnNLGyp47a/TdokrAhn1EeY3RSjdXGC1kVttC6fT9PiRcQWLMNvX8CLn/gFMJqEFej4gYbfXhehoT1OY1cjjZ0N7HxuH5mh/gIdP5tJ57X0sC7d2zg70PH7Mwyls/SnPAZSHkdTHgNpj/7hDINJj4FkoO3HE7PySVmRqEskltPxbQOUqIsTcYhEHQ7s7MPP+mQ9b5yGn7sP3752zqgb1e9tMlbUcYi6ktfzHce+2sJoxXT8YgXTGqJuQUG0sI4/qrtLSb35WDq/iIw2UQkd74zZZ7KMK7h2jHOc7OJrzkkW3lXHP4kYo5q+oihKLeGr0VcURakRNGRTURSldjCAX6VO2nJQo68oihLGGHXkVppZs7v48dc/xCu7Gmi+8G+om9FK8+wl+QSsS5d3snpuC2d3NNCa6cXZvZHU42s5vOFF3tTVSNv8ZloXJWhdPo+WpQuJLliGdJ9FtmUOvdkIPcMeu3qTdNS5BQlYTYl6GjsbrfN2BvGOBI3dbcTaWun90vqCBKyxjkhx3PyyqWd4XAJW/3Am77gdTAbvU+ks6ZRHU3t7QQKWE0rKciMSOHdtB6xdz+0Z57zNOW6Nn8VkR993zKwbl4AVdQOnbdTJdb0afZ/NpPPzmajbVdRxChKwwhU1C8ZLHH8sXOuxPZbj9ngdraWct+q4rV2MJmcpiqLUEGr0FUVRagnNyFUURakdpigjt5z+IiJyqYisCy1JEbnabrtXRHaEtq0s57pV8aTfOdJD3Yeu59dPHeDaz91TkHzV2P8S/o5nGP71Og5t2MauLT0c2XqEfUdT9KSyfOh7t+STr7yW2fQMe/QMeezsHWbXjtHuV729ST6zpC2ffNXQ2UxjdxvxjgTRtnbctlm4iU5oTODXzyT52X8ouMewhu9Eg05XTiSKE4nx251HCpKvBpJBMlY6nSWTygadrrI+XjqLnzU0tzXkk69yRdZySVV1EYd4LBKsuw5PDBwpqeGPdrsKnlpa66MFyVfumPeOEGj+7mhyVnB8cf09PB5xC5OvHBnV78NJW8c6XykcCrX3cUlVkzpb6LhjnHPcvpM898nW8MOonn9qMUxZnH6uv8idInK7Xb+t4F6MeRRYCcGXBLAN+FVol/9hjPnhZC5aFUZfURRlyjAGf2qid64iKFUDQX+R3zDG6I/hWuAXxpjhE7moyjuKoighjAme9MtZTpCy+4tYrgfuHzP2ORHZICJ3iUhdORfVJ31FUZQxTKIrVruIrA2t3217gQAgIg8Ds4ocd0fB9SboL2JL0Z8LrAkNf4zgyyJG0HvkNuAzE91wVRj9vXv6+NqeF4i7wjfnvcDw0z/i0L3b2LSlh77tfew7muJAMstRL2iAkvsCdgU2vuJd7OgbYdeWYbb3bGHXoSH6+5IMHU0yMpAmOTScL5y26u+uINrRhZvoGNXv48348RbSbl1QNC3jM+IZnEisqH7vRGNEYkGxNCcSw62L8/iWHlIpDy/t42Wshu/5eJkxjU9s4bTZq+YSizg0xFxiETev3+c0/XDjk/RQ/zj9vpgW7/tZEvFogX4fdZx8s5NizU/Cx0+kw8ecXIOTQv0+91OyWAOUcnFDB409/ETi6Usdq5J5jWMm9RR/yBizqvSpzOWltonIZPqLvAP4iTEmEzp37ldCSkS+CXyknBtWeUdRFCWMjdMvZzlBJtNf5AbGSDv2iwIJnqiuBjaWc9GqeNJXFEWZKgxTVnCtaH8REVkFvM8Yc5NdXwDMBR4bc/x9ItJB8ON0HfC+ci6qRl9RFCWMMWTTp97oG2MOU6S/iDFmLXBTaH0nMLvIfpcdz3XV6CuKooQwBnyjZRgqSsfMOj76l6+ldfkCPv/mT3HU8xnJjjpsY44Qd4WZEZe58SitMZdEY5R4ewM3/d8/MDyQIjU0GDhsh/rxkkP4XhovNZIvVAZQd91dpJ0Y/Rmf4YzPSMYwMOLR35umPzXMYDroeNU/nGHGGWdZB24MNxa3Dty6UFcrN18cbd/OPnzPzydhGd+Q9bygQFq2sMuV8bOsmL2ioLtVfgkVSYs6Dq6AlxzKO1n9sOO1SKerRDxa1GE7tjDaRElUxcajTu4chQ7bUp2uJoNQ3Ol6PN2yxp63XLRgWm2RVaOvKIpSGxjgNK63pkZfURRlLPqkryiKUiP4BtLaOauy+PPO5Im/+AI7jgzTGvsRi5piec2+qbOBxq5G4p0JGme1Eu9MEEl04LZ14yY62PLub+c1+zD54mg2gcqNxLhvc39esx9MevSNZBhMZhhOZxlMekFilU2wmrX07Lxmn2t44rh23TY4ySVTPfaLZwo0+2xOw8+OJlGFE6yWdc/Ia/YRN3gNtPzR97liaTm/RJhSWvzMukiBZp8rkDa2wUlOv55MYbSIKyWbnJxoQxJ3zAlOdoOT4Jyq2SujqLyjKIpSIxiMyjuKoii1gjpyFUVRagw1+hVm666X+asP/B/8TJqhp79ti6A149fNIOkLQxnDcMZnv+fTn/SCJuRpj8E+j3iia1whNLcueI3EooEeb2Prv/LTTbaZSVAEraAYWq7puG1CfsVV5+dj58cWQcvH2LsOUUd48N6dAOMKoZWKq1/c2njMQmjhZiXZ9MiE/3656zXFAtV9bBE0KB5XPxliZejuxxtXH27IcjKZjI6vGn3tYIxG7yiKotQMBo3eURRFqRlU01cURakxVN5RFEWpEQJNv9J3ceqoCqPvxurpPPsi3IjDJT8ewssM4GV22SJmvu1Clc13n/J9g++l8TNp3viuP7fOVZd41C3oPjW2oNknPnWvTZIKyqqWcryabJYbXv02HGGco7WY4zU1cKTgPBMxrzlodVlO96nJJFA1RoMzFfNJnmjCU9QtPMHJ9Hu6p8iLqs5ZpRT6pK8oilIjGGBKWqhUCDX6iqIoIQxGo3cURVFqhSB6R41+RVkxv5Xf/fNbAWi+8G8mdey9X7++7H1v7dld9r4XzZ1R9r7FCr4di87GU/NnaYgebxuTiYmciipoFtXelSnlNHfknjorcAxE5AoR2SIi20Tk9krcg6IoSjFyT/rlLCeCiFwnIs+JiG+boZfar6i9FJGFIvKkHf+eiMTKue6UG30RcYGvAG8GzgZuEJGzp/o+FEVRSpE15S0nyEbg7cDjpXaYwF5+HrjLGLMI6AVuLOeilXjSXw1sM8ZsN8akge8CV1XgPhRFUcbhE5RhKGc5EYwxzxtjtkywW1F7KUH89mXAD+1+3wKuLue6YqbYYSEi1wJXGGNusuv/FbjAGPP+MfvdDNxsV1cQfCueLrQDhyp9EyeR020+cPrNqZbmM98Y03G8JxaRX9rzl0M9kAyt322MuXuS1/sN8BFjzNoi24raS+DTwBP2KR8RmQv8whizYqLrTVtHrv2HuxtARNYaY0pqXtWGzmf6c7rNSedTPsaYK07WuUTkYWBWkU13GGN+erKuMxkqYfT3AnND63PsmKIoymmFMebyEzxFKXt5GGgRkYgxxmMSdrQSmv6fgMXW8xwDrgceqMB9KIqiTHeK2ksT6PKPAtfa/d4DlPXLYcqNvv1Wej+wBnge+L4x5rkJDpuURlYF6HymP6fbnHQ+0wwReZuI7AEuBB4UkTV2/AwR+TlMaC9vA24VkW1AG3BPWdedakeuoiiKUjkqkpylKIqiVAY1+oqiKDXEtDb61VquQUS+ISIHRWRjaKxVRB4Ska32NWHHRUT+2c5xg4icV7k7L46IzBWRR0Vkk00b/5Adr8o5iUi9iPxRRNbb+fy9HS+a1i4idXZ9m92+oJL3XwoRcUXkGRH5mV2v9vnsFJFnRWSdiKy1Y1X5mZtOTFujX+XlGu4Fxsb63g48YoxZDDxi1yGY32K73Ax8dYrucTJ4wIeNMWcDrwH+1v4tqnVOKeAyY8wrgZXAFSLyGkqntd8I9Nrxu+x+05EPETj7clT7fAAuNcasDMXkV+tnbvpgjJmWC4FHe01o/WPAxyp9X5O4/wXAxtD6FqDbvu8Gttj3XwNuKLbfdF0IQsPecDrMCWgAnibIcjwEROx4/vNHEDlxoX0fsftJpe99zDzmEBjBy4CfETQvq9r52HvbCbSPGav6z1yll2n7pA/MBsK1jvfYsWqlyxiz374/AHTZ91U1TysFvAp4kiqek5VC1gEHgYeAF4E+E4TIQeE95+djt/cThMhNJ/4R+CijTZ/aqO75QFDw8lci8pQtywJV/JmbLkzbMgynM8YYIyJVFysrIk3Aj4BbjDFHJVTovtrmZIzJAitFpAX4CbCswrd03IjIW4GDxpinROT1lb6fk8jFxpi9ItIJPCQim8Mbq+0zN12Yzk/6p1u5hpdFpBvAvh6041UxTxGJEhj8+4wxP7bDVT0nAGNMH0Fm44XYtHa7KXzP+fnY7c0EafDThYuAK0VkJ0EVxsuAf6J65wOAMWavfT1I8MW8mtPgM1dpprPRP93KNTxAkCoNhSnTDwB/YaMPXgP0h36+TgskeKS/B3jeGPPF0KaqnJOIdNgnfEQkTuCfeJ7Sae3heV4L/NpY4Xg6YIz5mDFmjjFmAcH/k18bY95Nlc4HQEQaRWRG7j3wRoJKu1X5mZtWVNqpcKwFeAvwAoHeekel72cS930/sB/IEGiLNxJopo8AW4GHgVa7rxBEKb0IPAusqvT9F5nPxQT66gZgnV3eUq1zAl4BPGPnsxH4pB0/E/gjsA34AVBnx+vt+ja7/cxKz+EYc3s98LNqn4+99/V2eS73/79aP3PTadEyDIqiKDXEdJZ3FEVRlJOMGn1FUZQaQo2+oihKDaFGX1EUpYZQo68oilJDqNFXKo6IZG0lxeds5csPi8hxfzZF5OOh9wskVO1UUWodNfrKdGDEBJUUzyFIlHoz8KkTON/HJ95FUWoTNfrKtMIEKfc3A++32ZWuiPwvEfmTrZP+1wAi8noReVxEHpSg58K/iIgjIncCcfvL4T57WldEvm5/SfzKZuEqSk2iRl+ZdhhjtgMu0EmQzdxvjDkfOB/4KxFZaHddDXyAoN/CWcDbjTG3M/rL4d12v8XAV+wviT7gmqmbjaJML9ToK9OdNxLUVFlHUM65jcCIA/zRGLPdBBUz7ycoF1GMHcaYdfb9UwS9DhSlJtHSysq0Q0TOBLIEFRQF+IAxZs2YfV5PUA8oTKmaIqnQ+yyg8o5Ss+iTvjKtEJEO4F+AL5ugMNQa4L/b0s6IyBJbdRFgta3C6gDvBH5rxzO5/RVFKUSf9JXpQNzKN1GCfrzfBnIlnP+VQI552pZ47gGuttv+BHwZWERQRvgndvxuYIOIPA3cMRUTUJRqQatsKlWJlXc+Yox5a6XvRVGqCZV3FEVRagh90lcURakh9ElfURSlhlCjryiKUkOo0VcURakh1OgriqLUEGr0FUVRaoj/D/HCIojbm3mzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = kargs['num_heads']\n",
    "        self.d_model = kargs['d_model']\n",
    "\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(**kargs):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(**kargs)\n",
    "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(**kargs)\n",
    "        self.mha2 = MultiHeadAttention(**kargs)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n",
    "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(**kargs) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n",
    "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(**kargs) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(**kargs)\n",
    "\n",
    "        self.decoder = Decoder(**kargs)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
    "\n",
    "    def call(self, x):\n",
    "#         inp, tar = x['enc_input'], x['dec_input']\n",
    "        inp, tar = x\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        self.attention_weights = attention_weights\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        return self.attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 실행 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kargs = {'num_layers': 2,\n",
    "         'd_model': 512,\n",
    "         'num_heads': 8,\n",
    "         'dff': 2048,\n",
    "         'input_vocab_size': vocab_size,\n",
    "         'target_vocab_size': vocab_size,\n",
    "         'maximum_position_encoding': MAX_SEQUENCE,\n",
    "         'rate': 0.1\n",
    "        }\n",
    "\n",
    "model = Transformer(**kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask    \n",
    "    acc = train_accuracy(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss_function,\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Transformer.call of <__main__.Transformer object at 0x7f409c160748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x7f409c160748>>, which Python reported as:\n",
      "    def call(self, x):\n",
      "#         inp, tar = x['enc_input'], x['dec_input']\n",
      "        inp, tar = x\n",
      "\n",
      "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
      "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
      "\n",
      "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
      "        dec_output, attention_weights = self.decoder(\n",
      "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
      "        self.attention_weights = attention_weights\n",
      "\n",
      "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Transformer.call of <__main__.Transformer object at 0x7f409c160748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x7f409c160748>>, which Python reported as:\n",
      "    def call(self, x):\n",
      "#         inp, tar = x['enc_input'], x['dec_input']\n",
      "        inp, tar = x\n",
      "\n",
      "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
      "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
      "\n",
      "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
      "        dec_output, attention_weights = self.decoder(\n",
      "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
      "        self.attention_weights = attention_weights\n",
      "\n",
      "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "Train on 18 samples, validate on 2 samples\n",
      "Epoch 1/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.9660 - accuracy_function: 0.8387WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 8s 471ms/sample - loss: 0.9294 - accuracy_function: 0.8388 - val_loss: 0.5874 - val_accuracy_function: 0.8440\n",
      "Epoch 2/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.6139 - accuracy_function: 0.8490WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 65ms/sample - loss: 0.6544 - accuracy_function: 0.8492 - val_loss: 0.6336 - val_accuracy_function: 0.8520\n",
      "Epoch 3/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.5699 - accuracy_function: 0.8536WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 64ms/sample - loss: 0.5440 - accuracy_function: 0.8540 - val_loss: 0.5821 - val_accuracy_function: 0.8580\n",
      "Epoch 4/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3378 - accuracy_function: 0.8641WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 64ms/sample - loss: 0.3646 - accuracy_function: 0.8643 - val_loss: 0.5610 - val_accuracy_function: 0.8665\n",
      "Epoch 5/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2224 - accuracy_function: 0.8710WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 77ms/sample - loss: 0.2215 - accuracy_function: 0.8717 - val_loss: 0.9522 - val_accuracy_function: 0.8768\n",
      "Epoch 6/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2181 - accuracy_function: 0.8810WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 77ms/sample - loss: 0.2210 - accuracy_function: 0.8815 - val_loss: 0.7348 - val_accuracy_function: 0.8853\n",
      "Epoch 7/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1517 - accuracy_function: 0.8904WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 74ms/sample - loss: 0.1552 - accuracy_function: 0.8908 - val_loss: 0.5467 - val_accuracy_function: 0.8943\n",
      "Epoch 8/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1370 - accuracy_function: 0.8983WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 65ms/sample - loss: 0.1323 - accuracy_function: 0.8988 - val_loss: 0.4934 - val_accuracy_function: 0.9025\n",
      "Epoch 9/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0770 - accuracy_function: 0.9070WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 72ms/sample - loss: 0.0757 - accuracy_function: 0.9074 - val_loss: 0.3589 - val_accuracy_function: 0.9104\n",
      "Epoch 10/10\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0537 - accuracy_function: 0.9140WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy_function,val_loss,val_accuracy_function\n",
      "18/18 [==============================] - 1s 67ms/sample - loss: 0.0603 - accuracy_function: 0.9144 - val_loss: 0.3382 - val_accuracy_function: 0.9168\n"
     ]
    }
   ],
   "source": [
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=1)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n",
    "\n",
    "history = model.fit([index_inputs, index_outputs], index_targets, \n",
    "                    batch_size=2, epochs=10,\n",
    "                    validation_split=0.1, callbacks=[earlystop_callback])\n",
    "\n",
    "\n",
    "# history = model.fit(dataset, epochs=2)#, callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력값 보이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"가끔 궁금해\"\n",
    "index_inputs, _ = enc_processing([text], char2idx)\n",
    "index_outputs, _ = dec_output_processing([\"\"], char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\n",
      "[100  34]\n",
      "[100  34  63]\n",
      "[100  34  63 103]\n",
      "[100  34  63 103   2]\n",
      "[100  34  63 103   2   2]\n",
      "[100  34  63 103   2   2   2]\n",
      "[100  34  63 103   2   2   2   2]\n",
      "[100  34  63 103   2   2   2   2   2]\n",
      "[100  34  63 103   2   2   2   2   2   2]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    prob_outputs = model((index_inputs, index_outputs[:, :i+1]))\n",
    "    outputs = tf.argmax(prob_outputs, -1).numpy()\n",
    "    print(outputs[0])\n",
    "#     if i >= 9 or outputs[0][i] == 2:\n",
    "#         break\n",
    "    \n",
    "    index_outputs[0][i+1] = outputs[0][i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그 사람도 그럴 거예요'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index = list(outputs[0]).index(2)\n",
    "' '.join([idx2char[o] for o in outputs[0][:end_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_layer1_block1': <tf.Tensor: id=22121, shape=(1, 8, 10, 10), dtype=float32, numpy=\n",
       " array([[[[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.82255816, 0.17744178, 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.8121237 , 0.06951626, 0.11835997, 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.7497528 , 0.11574161, 0.06566661, 0.0688389 , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.04822986, 0.06290007, 0.11141282, 0.22547609, 0.55198115,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.03699278, 0.02157314, 0.07418611, 0.15398979, 0.4278492 ,\n",
       "           0.28540906, 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.03123425, 0.01878298, 0.07733127, 0.16008563, 0.30850798,\n",
       "           0.21738288, 0.18667509, 0.        , 0.        , 0.        ],\n",
       "          [0.03630792, 0.01699315, 0.08808891, 0.1656925 , 0.2233821 ,\n",
       "           0.18691908, 0.16256593, 0.12005043, 0.        , 0.        ],\n",
       "          [0.04974546, 0.01561946, 0.09476794, 0.15257755, 0.1693381 ,\n",
       "           0.16603021, 0.14541778, 0.11238328, 0.09412021, 0.        ],\n",
       "          [0.0679336 , 0.01543486, 0.09363604, 0.12484632, 0.14436033,\n",
       "           0.1411502 , 0.12396485, 0.10072539, 0.09063579, 0.09731265]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.02588181, 0.9741182 , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.07262952, 0.35164672, 0.5757238 , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01968689, 0.15773089, 0.20783226, 0.6147499 , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01097375, 0.24280077, 0.5719183 , 0.13915469, 0.03515248,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.02000333, 0.24468642, 0.263794  , 0.22631223, 0.03023066,\n",
       "           0.21497339, 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01618558, 0.17654341, 0.24567099, 0.24572113, 0.01803007,\n",
       "           0.14722483, 0.15062398, 0.        , 0.        , 0.        ],\n",
       "          [0.01375686, 0.14619395, 0.24982439, 0.2715187 , 0.01284483,\n",
       "           0.10244635, 0.1025639 , 0.10085098, 0.        , 0.        ],\n",
       "          [0.01297022, 0.13960779, 0.2501946 , 0.27987054, 0.01220307,\n",
       "           0.08111361, 0.07717618, 0.07415561, 0.07270832, 0.        ],\n",
       "          [0.01323712, 0.14158773, 0.23195235, 0.2555297 , 0.01484105,\n",
       "           0.07804517, 0.07046909, 0.06543514, 0.06348249, 0.0654202 ]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.93023324, 0.06976672, 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.5791636 , 0.3098689 , 0.11096743, 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.16738711, 0.45895407, 0.20713487, 0.16652401, 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00908547, 0.01507593, 0.03169349, 0.1603626 , 0.78378254,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01489717, 0.031066  , 0.04362474, 0.08589385, 0.1635867 ,\n",
       "           0.66093147, 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00952327, 0.02135899, 0.03075564, 0.07300039, 0.1131496 ,\n",
       "           0.36414665, 0.3880655 , 0.        , 0.        , 0.        ],\n",
       "          [0.00923914, 0.02197654, 0.03011824, 0.07996772, 0.10269834,\n",
       "           0.26226002, 0.27711567, 0.21662422, 0.        , 0.        ],\n",
       "          [0.00972222, 0.02597221, 0.03292861, 0.08678354, 0.09897071,\n",
       "           0.2098646 , 0.22293946, 0.17835769, 0.13446106, 0.        ],\n",
       "          [0.01008155, 0.02983956, 0.03591464, 0.08463731, 0.09135381,\n",
       "           0.17480144, 0.18797337, 0.1555571 , 0.12162104, 0.10822013]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.06991792, 0.930082  , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.85823053, 0.09818954, 0.04357987, 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.08290844, 0.15814437, 0.03277854, 0.72616863, 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.24349065, 0.31882843, 0.11472062, 0.18838228, 0.13457803,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.0926985 , 0.12658516, 0.06607518, 0.2419724 , 0.15501668,\n",
       "           0.3176521 , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.08119003, 0.13023615, 0.07745606, 0.16024451, 0.13245985,\n",
       "           0.22757047, 0.19084302, 0.        , 0.        , 0.        ],\n",
       "          [0.06176023, 0.14078099, 0.08691446, 0.12440416, 0.10952315,\n",
       "           0.18149   , 0.15801942, 0.13710755, 0.        , 0.        ],\n",
       "          [0.04581977, 0.14256011, 0.08136965, 0.109747  , 0.08750855,\n",
       "           0.15373242, 0.13988344, 0.12705311, 0.11232597, 0.        ],\n",
       "          [0.03895413, 0.1303804 , 0.06567325, 0.10654354, 0.07354451,\n",
       "           0.13604295, 0.12735386, 0.12011614, 0.10920223, 0.09218904]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.7734056 , 0.22659433, 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.0563429 , 0.5049195 , 0.43873754, 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00180929, 0.00384066, 0.02582754, 0.96852255, 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00280571, 0.01467409, 0.06330891, 0.15390876, 0.7653026 ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01055406, 0.01888018, 0.08299174, 0.22380373, 0.2099875 ,\n",
       "           0.45378286, 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01080422, 0.01535448, 0.07605524, 0.14542091, 0.14760014,\n",
       "           0.32927993, 0.275485  , 0.        , 0.        , 0.        ],\n",
       "          [0.01205755, 0.01251739, 0.06769478, 0.12713668, 0.12237618,\n",
       "           0.26289958, 0.22441937, 0.17089847, 0.        , 0.        ],\n",
       "          [0.0125396 , 0.01037067, 0.05723698, 0.1427031 , 0.12108515,\n",
       "           0.22954687, 0.18996067, 0.13693175, 0.09962519, 0.        ],\n",
       "          [0.01161899, 0.00916429, 0.0468974 , 0.17675295, 0.12861097,\n",
       "           0.20939529, 0.16388166, 0.11162411, 0.0787028 , 0.06335152]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01315905, 0.98684096, 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00889642, 0.5565515 , 0.43455213, 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01385495, 0.5925679 , 0.32296595, 0.0706111 , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01073768, 0.16436917, 0.39114025, 0.1324288 , 0.3013241 ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00666162, 0.12399596, 0.5557287 , 0.0981366 , 0.14009511,\n",
       "           0.07538196, 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00851955, 0.1026831 , 0.5319824 , 0.09010119, 0.1289931 ,\n",
       "           0.06767995, 0.07004073, 0.        , 0.        , 0.        ],\n",
       "          [0.01094088, 0.09718776, 0.5020102 , 0.08564174, 0.12758835,\n",
       "           0.05383464, 0.05979741, 0.062999  , 0.        , 0.        ],\n",
       "          [0.01329133, 0.1018605 , 0.46814364, 0.08283194, 0.13512146,\n",
       "           0.04282792, 0.04942549, 0.05433738, 0.05216026, 0.        ],\n",
       "          [0.0146281 , 0.10781148, 0.42768824, 0.07810391, 0.14662443,\n",
       "           0.03958995, 0.045821  , 0.05051664, 0.04859988, 0.04061646]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.3001197 , 0.69988024, 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.04281744, 0.16525868, 0.7919239 , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.0118376 , 0.6934008 , 0.15658858, 0.13817307, 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.02220704, 0.16608517, 0.5118805 , 0.17450468, 0.12532255,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.02735339, 0.2827087 , 0.16695252, 0.1108928 , 0.27311835,\n",
       "           0.13897425, 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.03002999, 0.2783193 , 0.13901925, 0.10192199, 0.22287329,\n",
       "           0.12562308, 0.10221309, 0.        , 0.        , 0.        ],\n",
       "          [0.03063796, 0.30452684, 0.11552696, 0.0892656 , 0.20520957,\n",
       "           0.1005955 , 0.08453795, 0.06969967, 0.        , 0.        ],\n",
       "          [0.02809474, 0.32309258, 0.0984028 , 0.07673312, 0.2164152 ,\n",
       "           0.07723132, 0.06793628, 0.05784754, 0.05424638, 0.        ],\n",
       "          [0.02429565, 0.30125853, 0.08970691, 0.06999069, 0.24414127,\n",
       "           0.06381077, 0.05833515, 0.05063087, 0.04726664, 0.05056348]],\n",
       " \n",
       "         [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.07842595, 0.921574  , 0.        , 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.35161397, 0.33305958, 0.31532642, 0.        , 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.18500154, 0.06404261, 0.31070793, 0.44024786, 0.        ,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.06899495, 0.05881269, 0.17760308, 0.28508517, 0.40950412,\n",
       "           0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.02332507, 0.06724857, 0.2172533 , 0.15370564, 0.23740064,\n",
       "           0.3010668 , 0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01687014, 0.04292539, 0.1366515 , 0.09637166, 0.18805516,\n",
       "           0.27702552, 0.2421006 , 0.        , 0.        , 0.        ],\n",
       "          [0.01309973, 0.03031237, 0.08158949, 0.05603551, 0.14657846,\n",
       "           0.26064888, 0.22881462, 0.18292093, 0.        , 0.        ],\n",
       "          [0.01100704, 0.02431349, 0.05378304, 0.03677339, 0.11718489,\n",
       "           0.23623826, 0.20759957, 0.16584401, 0.14725627, 0.        ],\n",
       "          [0.00987886, 0.02191108, 0.04480628, 0.03149713, 0.10289138,\n",
       "           0.20670754, 0.18034092, 0.14323907, 0.12709458, 0.13163315]]]],\n",
       "       dtype=float32)>,\n",
       " 'decoder_layer1_block2': <tf.Tensor: id=22240, shape=(1, 8, 10, 25), dtype=float32, numpy=\n",
       " array([[[[0.5114356 , 0.48856434, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.6859672 , 0.31403273, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.737541  , 0.26245898, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.38358867, 0.6164113 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.39341307, 0.60658693, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.389975  , 0.6100249 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.6698869 , 0.33011308, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.8474855 , 0.15251452, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.76158917, 0.23841082, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.46906507, 0.53093493, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.47333676, 0.52666324, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5050835 , 0.49491644, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.59499276, 0.4050072 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.7368775 , 0.26312256, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.609861  , 0.39013892, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.49314544, 0.5068546 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.49473354, 0.5052665 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5026361 , 0.49736395, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.7527224 , 0.24727759, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.68892425, 0.31107578, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.59033287, 0.40966716, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.50539213, 0.49460787, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5042834 , 0.49571648, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5048336 , 0.49516636, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.12946211, 0.8705378 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.69897443, 0.30102557, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5787828 , 0.42121723, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.6836429 , 0.31635714, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.670959  , 0.32904097, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.66853565, 0.33146438, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.42277563, 0.5772244 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.40052977, 0.59947026, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.20756802, 0.79243195, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.46347734, 0.53652257, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.47610298, 0.52389705, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.49370462, 0.5062954 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]]]], dtype=float32)>,\n",
       " 'decoder_layer2_block1': <tf.Tensor: id=22405, shape=(1, 8, 10, 10), dtype=float32, numpy=\n",
       " array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [6.67142153e-01, 3.32857847e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [4.47010137e-02, 2.03832954e-01, 7.51466036e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.94335764e-02, 7.57867098e-02, 6.33373916e-01,\n",
       "           2.61405766e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.30774842e-03, 8.92869849e-03, 2.96090413e-02,\n",
       "           8.00375417e-02, 8.79116952e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.14785291e-03, 1.18960096e-02, 3.61219794e-02,\n",
       "           4.74249423e-02, 7.32806683e-01, 1.69602558e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.71809008e-03, 1.16846813e-02, 4.39893045e-02,\n",
       "           4.32215333e-02, 6.23922765e-01, 1.48459032e-01,\n",
       "           1.26004517e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.19788721e-03, 1.14730578e-02, 5.43416776e-02,\n",
       "           4.10510376e-02, 5.54887652e-01, 1.27063245e-01,\n",
       "           1.07035644e-01, 1.00949809e-01, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.40715959e-03, 1.15930345e-02, 6.48964345e-02,\n",
       "           4.07502241e-02, 5.14115572e-01, 1.06461078e-01,\n",
       "           8.82006809e-02, 8.21968168e-02, 8.83791149e-02,\n",
       "           0.00000000e+00],\n",
       "          [3.10917012e-03, 1.15873264e-02, 6.61349818e-02,\n",
       "           3.98138911e-02, 4.92735952e-01, 9.03242454e-02,\n",
       "           7.36870989e-02, 6.79191425e-02, 7.27547184e-02,\n",
       "           8.19334388e-02]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [6.45190537e-01, 3.54809463e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [7.19920546e-02, 1.68430537e-01, 7.59577334e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.14912987e-01, 1.10264130e-01, 1.42339557e-01,\n",
       "           6.32483304e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [5.45675270e-02, 6.80561289e-02, 5.93689494e-02,\n",
       "           6.08478189e-01, 2.09529251e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.09414584e-02, 3.61779481e-02, 3.56568247e-02,\n",
       "           5.72262585e-01, 1.03948742e-01, 2.21012518e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.44773868e-02, 2.80429292e-02, 2.83712801e-02,\n",
       "           5.18252790e-01, 8.96371827e-02, 1.58390939e-01,\n",
       "           1.52827516e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.00997312e-02, 2.37695426e-02, 2.55498774e-02,\n",
       "           4.84250873e-01, 8.59804973e-02, 1.30957484e-01,\n",
       "           1.25297591e-01, 1.04094356e-01, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.81279350e-02, 2.30067261e-02, 2.44527720e-02,\n",
       "           4.60090220e-01, 8.67414474e-02, 1.17993563e-01,\n",
       "           1.10022835e-01, 8.97038579e-02, 6.98606148e-02,\n",
       "           0.00000000e+00],\n",
       "          [1.91432666e-02, 2.55804639e-02, 2.34198123e-02,\n",
       "           4.47918236e-01, 8.77556577e-02, 1.06775664e-01,\n",
       "           9.74529609e-02, 7.89660215e-02, 6.10867701e-02,\n",
       "           5.19011691e-02]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.83941334e-01, 8.16058636e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.17671518e-02, 6.94060773e-02, 9.18826759e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [5.04541444e-04, 8.38821568e-03, 1.48875237e-01,\n",
       "           8.42231989e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [4.00394574e-03, 1.15023619e-02, 1.05763078e-01,\n",
       "           3.56278718e-01, 5.22451818e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.05677673e-03, 4.91217012e-03, 1.14194326e-01,\n",
       "           8.85594040e-02, 5.14472485e-01, 2.76804894e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.21674198e-03, 4.48388746e-03, 7.76128694e-02,\n",
       "           7.27711543e-02, 4.19460863e-01, 2.27753937e-01,\n",
       "           1.96700633e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.38943666e-03, 4.61213198e-03, 6.33620694e-02,\n",
       "           6.29368573e-02, 3.70590299e-01, 1.84102178e-01,\n",
       "           1.64696887e-01, 1.48310170e-01, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.54913811e-03, 5.00945887e-03, 6.06563687e-02,\n",
       "           5.79123534e-02, 3.30140054e-01, 1.50808960e-01,\n",
       "           1.37688920e-01, 1.26873285e-01, 1.29361406e-01,\n",
       "           0.00000000e+00],\n",
       "          [1.72441418e-03, 5.46148699e-03, 6.09915145e-02,\n",
       "           5.54104373e-02, 2.81781971e-01, 1.28855705e-01,\n",
       "           1.18325070e-01, 1.10064499e-01, 1.13361560e-01,\n",
       "           1.24023244e-01]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [4.67374444e-01, 5.32625556e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [9.01938900e-02, 9.64632705e-02, 8.13342869e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.63430776e-02, 3.28704655e-01, 1.07332267e-01,\n",
       "           5.47619998e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [5.25753479e-03, 9.12190899e-02, 2.15834007e-01,\n",
       "           3.02959532e-01, 3.84729832e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.30830186e-03, 9.07579139e-02, 5.15629277e-02,\n",
       "           1.75844669e-01, 2.65678018e-01, 4.13848251e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.78624538e-03, 7.56617412e-02, 4.72932123e-02,\n",
       "           1.45312458e-01, 2.10804686e-01, 3.20902050e-01,\n",
       "           1.98239625e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.40820455e-03, 7.29829147e-02, 4.62580509e-02,\n",
       "           1.36650413e-01, 1.78294256e-01, 2.72322714e-01,\n",
       "           1.66129678e-01, 1.25953838e-01, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.05106656e-03, 7.14895874e-02, 4.35342342e-02,\n",
       "           1.30248219e-01, 1.54272631e-01, 2.38403693e-01,\n",
       "           1.45313546e-01, 1.09693408e-01, 1.05993576e-01,\n",
       "           0.00000000e+00],\n",
       "          [7.78240617e-04, 6.61507025e-02, 3.71031351e-02,\n",
       "           1.13012053e-01, 1.28756136e-01, 2.10215583e-01,\n",
       "           1.31826282e-01, 1.02298073e-01, 1.01039134e-01,\n",
       "           1.08820654e-01]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [7.29244947e-01, 2.70755053e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.18791336e-01, 4.97144639e-01, 2.84063935e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.42307326e-01, 1.46750733e-01, 5.25397420e-01,\n",
       "           1.85544506e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.00759768e-02, 2.10752431e-02, 5.67288399e-02,\n",
       "           1.78905398e-01, 7.23214567e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [4.64498764e-03, 1.12724165e-02, 4.60343733e-02,\n",
       "           1.76231191e-01, 6.30269289e-01, 1.31547779e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.68047366e-03, 8.96823034e-03, 4.27094325e-02,\n",
       "           1.34157717e-01, 5.98457634e-01, 1.18642934e-01,\n",
       "           9.33836922e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.56116495e-03, 7.65656820e-03, 4.41637039e-02,\n",
       "           1.11289755e-01, 5.78902185e-01, 1.09132670e-01,\n",
       "           8.49473402e-02, 6.03466779e-02, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.91160510e-03, 7.27985334e-03, 5.13379499e-02,\n",
       "           1.09747760e-01, 5.46808183e-01, 1.01650201e-01,\n",
       "           7.86834210e-02, 5.66756763e-02, 4.39053103e-02,\n",
       "           0.00000000e+00],\n",
       "          [4.04080981e-03, 7.52009591e-03, 5.94687760e-02,\n",
       "           1.21472970e-01, 5.08460701e-01, 9.44816694e-02,\n",
       "           7.24885613e-02, 5.25480732e-02, 4.13311571e-02,\n",
       "           3.81872579e-02]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.36431898e-02, 9.66356814e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.01423615e-01, 3.71538818e-01, 4.27037567e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [8.92030522e-02, 1.90610066e-01, 4.33851272e-01,\n",
       "           2.86335647e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [3.55306896e-03, 2.89358962e-02, 7.82987550e-02,\n",
       "           2.19610795e-01, 6.69601500e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [9.64445469e-04, 3.26091088e-02, 1.87473819e-01,\n",
       "           6.33248761e-02, 5.60178578e-01, 1.55449271e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [9.29022033e-04, 2.91960388e-02, 1.78869128e-01,\n",
       "           5.03220819e-02, 4.87918884e-01, 1.28515944e-01,\n",
       "           1.24248952e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [9.83868842e-04, 2.99489573e-02, 1.82141900e-01,\n",
       "           4.46583256e-02, 4.24309492e-01, 1.10366479e-01,\n",
       "           1.12236738e-01, 9.53542665e-02, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.22331246e-03, 3.34542543e-02, 1.95527151e-01,\n",
       "           4.45422679e-02, 3.73202235e-01, 9.48667303e-02,\n",
       "           9.95006710e-02, 8.71524364e-02, 7.05309361e-02,\n",
       "           0.00000000e+00],\n",
       "          [1.71351782e-03, 3.90282609e-02, 2.13386983e-01,\n",
       "           4.99932058e-02, 3.45173270e-01, 7.99340308e-02,\n",
       "           8.41632485e-02, 7.49865472e-02, 6.18485808e-02,\n",
       "           4.97722924e-02]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [5.93330443e-01, 4.06669587e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.29632086e-01, 2.14928418e-01, 6.55439496e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.47656985e-02, 1.35849610e-01, 5.55292107e-02,\n",
       "           7.93855488e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.11695046e-03, 4.13113758e-02, 3.51432636e-02,\n",
       "           1.60331175e-01, 7.61097372e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [7.64412980e-04, 4.29688431e-02, 4.48281094e-02,\n",
       "           1.35606557e-01, 5.52168310e-01, 2.23663732e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [8.23643000e-04, 3.81462239e-02, 3.83003764e-02,\n",
       "           1.00847200e-01, 5.28003454e-01, 1.59507453e-01,\n",
       "           1.34371608e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.12921360e-03, 3.67821455e-02, 3.63200270e-02,\n",
       "           8.51131454e-02, 4.88028288e-01, 1.29778296e-01,\n",
       "           1.12561844e-01, 1.10287033e-01, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.58847752e-03, 3.37696038e-02, 3.64803933e-02,\n",
       "           7.92374760e-02, 4.04219568e-01, 1.17966913e-01,\n",
       "           1.06135607e-01, 1.07348040e-01, 1.13253951e-01,\n",
       "           0.00000000e+00],\n",
       "          [1.79030385e-03, 2.80353855e-02, 3.50149609e-02,\n",
       "           7.54459873e-02, 3.10682565e-01, 1.11241907e-01,\n",
       "           1.02681197e-01, 1.06574148e-01, 1.14302859e-01,\n",
       "           1.14230745e-01]],\n",
       " \n",
       "         [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [8.04415811e-03, 9.91955876e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [7.20682144e-02, 7.44883060e-01, 1.83048666e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [7.65543804e-02, 2.79376447e-01, 1.81053415e-01,\n",
       "           4.63015735e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.86984036e-02, 1.01945385e-01, 9.89127085e-02,\n",
       "           1.21421799e-01, 6.59021676e-01, 0.00000000e+00,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [2.53026299e-02, 1.34302825e-01, 1.08829372e-01,\n",
       "           1.00646205e-01, 3.91736209e-01, 2.39182711e-01,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.92867052e-02, 1.04227059e-01, 8.59974846e-02,\n",
       "           8.89946595e-02, 3.41554314e-01, 1.97941378e-01,\n",
       "           1.61998376e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [1.27855884e-02, 8.10622945e-02, 6.42987713e-02,\n",
       "           8.04192945e-02, 3.09340745e-01, 1.84862792e-01,\n",
       "           1.50724456e-01, 1.16505980e-01, 0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "          [9.34918504e-03, 6.58646449e-02, 5.03074452e-02,\n",
       "           7.48560354e-02, 2.74498791e-01, 1.79238454e-01,\n",
       "           1.46036297e-01, 1.10672407e-01, 8.91767740e-02,\n",
       "           0.00000000e+00],\n",
       "          [9.09543969e-03, 5.95759824e-02, 4.46283035e-02,\n",
       "           7.38331079e-02, 2.48170212e-01, 1.68723866e-01,\n",
       "           1.37539148e-01, 1.03526875e-01, 8.16733763e-02,\n",
       "           7.32336193e-02]]]], dtype=float32)>,\n",
       " 'decoder_layer2_block2': <tf.Tensor: id=22524, shape=(1, 8, 10, 25), dtype=float32, numpy=\n",
       " array([[[[0.337587  , 0.66241306, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.32851472, 0.6714853 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.40276322, 0.5972368 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.6245671 , 0.3754329 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5988884 , 0.40111163, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5887633 , 0.41123673, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.48443377, 0.5155663 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.54779804, 0.45220193, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5108104 , 0.48918957, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.6347064 , 0.3652936 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.623556  , 0.37644398, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.62534195, 0.37465814, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.5233676 , 0.4766324 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5493163 , 0.45068374, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5300635 , 0.46993643, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.5171115 , 0.48288855, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5284501 , 0.4715499 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5298232 , 0.4701768 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.3812096 , 0.6187904 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.7500179 , 0.24998213, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.39586374, 0.6041362 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.49980158, 0.5001984 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.48789564, 0.51210433, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.47994652, 0.52005345, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.52014786, 0.47985214, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.5430657 , 0.45693427, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.6051985 , 0.39480147, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.44307506, 0.55692494, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.4499447 , 0.55005527, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.47586307, 0.52413696, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.39789325, 0.6021067 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.21094483, 0.78905517, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.6778704 , 0.32212958, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.732281  , 0.26771897, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.72051215, 0.27948788, 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.7115512 , 0.2884488 , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]]]], dtype=float32)>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_attention_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
