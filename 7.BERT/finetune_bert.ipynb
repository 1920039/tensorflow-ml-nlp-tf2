{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tokenization_bert import BertTokenizer\n",
    "from modeling_tf_bert import TFBertForSequenceClassification\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import wget\n",
    "\n",
    "# if you have gpu\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'bert_ckpt'\n",
    "vocab_path = os.path.join(ckpt_path, 'bert-base-multilingual-uncased-vocab.txt')\n",
    "config_path = os.path.join(ckpt_path, 'config.json')\n",
    "model_path = os.path.join(ckpt_path, 'tf_model.h5')\n",
    "\n",
    "# Vocab 파일 불러오기\n",
    "if os.path.isfile(vocab_path):\n",
    "    print(\"vocab exists\")\n",
    "    tokenizer = BertTokenizer(vocab_path) #토크나이저 불러오기\n",
    "else:\n",
    "    print(\"vocab does not exists\")\n",
    "    wget.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\", ckpt_path)\n",
    "    \n",
    "#BERT Config파일 불러오기\n",
    "if os.path.isfile(config_path):\n",
    "    print(\"Config model exists\")\n",
    "else:\n",
    "    print(\"Config model does not exists\")\n",
    "    wget.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\", ckpt_path)\n",
    "    \n",
    "#BERT 모델 불러오기, huggingface의 저장된 로컬 모델을 불러오려면 아래와 같은 방법으로 접근해야 가능.\n",
    "if os.path.isfile(model_path):\n",
    "    print(\"Pretrained model exists\")\n",
    "    model = TFBertForSequenceClassification.from_pretrained(ckpt_path) # 모델 학습 불러오기\n",
    "else:\n",
    "    print(\"Pretrained model does not exists\")\n",
    "    wget.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-tf_model.h5\", ckpt_path)\n",
    "    os.rename(os.path.join(ckpt_path, 'bert-base-multilingual-uncased-config.json'), config_path) # 'bert-base-multilingual-uncased-config.json' -> config.json\n",
    "    os.rename(os.path.join(ckpt_path, 'bert-base-multilingual-uncased-tf_model.h5'), model_path) # 'bert-base-multilingual-uncased-tf_model.h5' -> tf_model.h5\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_path) #토크나이저 불러오기\n",
    "model = TFBertForSequenceClassification.from_pretrained(ckpt_path) # 모델 학습 불러오기\n",
    "    \n",
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "\n",
    "# bert_checkpoint_path = \"larva-kor-plus-base-cased-pytorch/\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(bert_checkpoint_path)\n",
    "# model = TFBertForSequenceClassification.from_pretrained(bert_checkpoint_path, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 50\n",
    "DATA_OUT_PATH = \"data_out/\"\n",
    "model_name = \"tf2_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[SEP]', '[CLS]', '[UNK]', '[MASK]', '[PAD]'] \n",
      " [102, 101, 100, 103, 0]\n",
      "[101, 1174, 26646, 49345, 13045, 35132, 25169, 47024, 117, 1170, 26646, 11376, 17360, 13212, 79427, 102]\n",
      "[101, 29155, 10228, 102]\n",
      "[CLS] 안녕하세요, 반갑습니다 [SEP]\n",
      "[CLS] hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Special Tokens\n",
    "print(tokenizer.all_special_tokens, \"\\n\", tokenizer.all_special_ids)\n",
    "\n",
    "# Test Tokenizers\n",
    "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다\")\n",
    "eng_encode = tokenizer.encode(\"Hello world\")\n",
    "\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "\n",
    "print(kor_encode)\n",
    "print(eng_encode)\n",
    "print(kor_decode)\n",
    "print(eng_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 준비\n",
    "\n",
    "DATA_PATH = 'data_in/'\n",
    "DATA_TRAIN_PATH = DATA_PATH + \"ratings_train.txt\"\n",
    "DATA_TEST_PATH = DATA_PATH + \"ratings_test.txt\"\n",
    "\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", sent)\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or bytes-like object\n",
      "nan\n",
      "expected string or bytes-like object\n",
      "nan\n",
      "expected string or bytes-like object\n",
      "nan\n",
      "expected string or bytes-like object\n",
      "nan\n",
      "expected string or bytes-like object\n",
      "nan\n",
      "num sents, labels 149995, 149995\n"
     ]
    }
   ],
   "source": [
    "# train_data = train_data[:1000] # for test\n",
    "\n",
    "train_data_sents = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in zip(train_data[\"document\"], train_data[\"label\"]):\n",
    "    try:\n",
    "        token_sent = tokenizer.encode(clean_text(train_sent))\n",
    "        train_data_sents.append(token_sent)\n",
    "        train_data_labels.append(train_label)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "\n",
    "train_data_sent_pads = np.asarray(pad_sequences(train_data_sents, maxlen=MAX_LEN, padding='post'), dtype=np.int32) # convert into numpy\n",
    "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
    "\n",
    "print(\"num sents, labels {}, {}\".format(len(train_data_sent_pads), len(train_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  1174 25539 23236 29234 13045 87550 97082 25539  1176 25539 24937\n",
      " 13045 16801 72197 47024  1169 70724 22585 13926   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_sent_pads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 준비하기\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 167,357,954\n",
      "Trainable params: 167,357,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained(\"bert_checkpoint/\") # 모델 프리트레인 저장하기\n",
    "model.summary() #모델 파라메터 수 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/tf2_bert -- Folder already exists \n",
      "\n",
      "Train on 119996 samples, validate on 29999 samples\n",
      "Epoch 1/20\n",
      "119808/119996 [============================>.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9588\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84766, saving model to data_out/tf2_bert/weights.01-0.85.h5\n",
      "119996/119996 [==============================] - 387s 3ms/sample - loss: 0.1055 - accuracy: 0.9588 - val_loss: 0.0089 - val_accuracy: 0.8477\n",
      "Epoch 2/20\n",
      "119808/119996 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9664\n",
      "Epoch 00002: val_accuracy did not improve from 0.84766\n",
      "119996/119996 [==============================] - 382s 3ms/sample - loss: 0.0865 - accuracy: 0.9664 - val_loss: 0.0096 - val_accuracy: 0.8457\n",
      "Epoch 3/20\n",
      " 90368/119996 [=====================>........] - ETA: 1:34 - loss: 0.0846 - accuracy: 0.9673"
     ]
    }
   ],
   "source": [
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=5)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.{epoch:02d}-{val_accuracy:.2f}.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# 학습과 eval 시작\n",
    "history = model.fit(train_data_sent_pads, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback], validation_steps=2)\n",
    "\n",
    "#steps_for_epoch\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(DATA_TEST_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sents = []\n",
    "test_data_labels = []\n",
    "\n",
    "for test_sent, test_label in zip(test_data[\"document\"], test_data[\"label\"]):\n",
    "    try:\n",
    "        token_sent = tokenizer.encode(clean_text(test_sent))\n",
    "        test_data_sents.append(token_sent)\n",
    "        test_data_labels.append(test_label)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(test_sent)\n",
    "        pass\n",
    "\n",
    "test_data_sent_pad = np.asarray(pad_sequences(test_data_sents, maxlen=MAX_LEN, padding='post'), dtype=np.int32) # convert into numpy\n",
    "test_data_labels = np.asarray(test_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
    "\n",
    "print(\"num sents, labels {}, {}\".format(len(test_data_sent_pad), len(test_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data_sent_pad, test_data_labels)\n",
    "print(\"test loss, test acc: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
