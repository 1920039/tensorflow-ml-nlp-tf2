{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no setGPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "#     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    import setGPU\n",
    "except:\n",
    "    print('no setGPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 128 # EDA에서 추출된 Max Length\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = \"data_out/KOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 준비\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"train.tsv\")\n",
    "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 학습 데이터 개수: 81000\n"
     ]
    }
   ],
   "source": [
    "def read_file(input_path):\n",
    "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
    "\n",
    "train_ner_dict = {\"sentence\": train_sentences, \"label\": train_labels}\n",
    "train_ner_df = pd.DataFrame(train_ner_dict)\n",
    "\n",
    "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 레이블 개수: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " 'O',\n",
       " 'PER-B',\n",
       " 'PER-I',\n",
       " 'FLD-B',\n",
       " 'FLD-I',\n",
       " 'AFW-B',\n",
       " 'AFW-I',\n",
       " 'ORG-B',\n",
       " 'ORG-I',\n",
       " 'LOC-B',\n",
       " 'LOC-I',\n",
       " 'CVL-B',\n",
       " 'CVL-I',\n",
       " 'DAT-B',\n",
       " 'DAT-I',\n",
       " 'TIM-B',\n",
       " 'TIM-I',\n",
       " 'NUM-B',\n",
       " 'NUM-I',\n",
       " 'EVT-B',\n",
       " 'EVT-I',\n",
       " 'ANM-B',\n",
       " 'ANM-I',\n",
       " 'PLT-B',\n",
       " 'PLT-I',\n",
       " 'MAT-B',\n",
       " 'MAT-I',\n",
       " 'TRM-B',\n",
       " 'TRM-I']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label 불러오기\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]\n",
    "\n",
    "ner_labels = get_labels(DATA_LABEL_PATH)\n",
    "\n",
    "print(\"개체명 인식 레이블 개수: {}\".format(len(ner_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ner_df = train_ner_df[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 토크나이저 설정\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 1\n",
    "sep_token_label_id = 2\n",
    "# pad_token_label_id=-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        truncation=True,\n",
    "        add_special_tokens = True, #'[CLS]'와 '[SEP]' 추가\n",
    "        max_length = MAX_LEN,           # 문장 패딩 및 자르기 진행\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True   # 어탠션 마스크 생성\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "def convert_label(words, labels_idx, ner_begin_label, max_seq_len):\n",
    "            \n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "\n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]\n",
    "        tokens.extend(word_tokens)\n",
    "        \n",
    "        # 슬롯 레이블 값이 Begin이면 I로 추가\n",
    "        if int(slot_label) in ner_begin_label:\n",
    "            label_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(word_tokens) - 1))\n",
    "        else:\n",
    "            label_ids.extend([int(slot_label)] * len(word_tokens))\n",
    "                    \n",
    "#         label_ids.extend([int(slot_label)] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "  \n",
    "    # [CLS] and [SEP] 설정\n",
    "    special_tokens_count = 2\n",
    "    if len(label_ids) > max_seq_len - special_tokens_count:\n",
    "        label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # [SEP] 토큰 추가\n",
    "    label_ids += [sep_token_label_id]\n",
    "\n",
    "    # [CLS] 토큰 추가\n",
    "    label_ids = [cls_token_label_id] + label_ids\n",
    "    \n",
    "    padding_length = max_seq_len - len(label_ids)\n",
    "    label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "    \n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "train_token_type_ids = []\n",
    "train_labels = []\n",
    "\n",
    "for i, data in enumerate(train_ner_df[['sentence', 'label']].values):\n",
    "    sentence, labels = data\n",
    "    words = sentence.split()\n",
    "    labels = labels.split()\n",
    "    \n",
    "#     print(words)\n",
    "#     print(labels)\n",
    "    \n",
    "    labels_idx = []\n",
    "    for label in labels:\n",
    "        labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index(\"UNK\"))\n",
    "        \n",
    "    ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "    assert len(words) == len(labels_idx)\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids = bert_tokenizer(sentence, MAX_LEN)\n",
    "    \n",
    "    convert_label_ids = convert_label(words, labels_idx, ner_begin_label, MAX_LEN)\n",
    "    \n",
    "#     print(input_ids)\n",
    "#     print(convert_label_ids)\n",
    "    \n",
    "    train_input_ids.append(input_ids)\n",
    "    train_attention_masks.append(attention_mask)\n",
    "    train_token_type_ids.append(token_type_ids)\n",
    "    train_labels.append(convert_label_ids)\n",
    "    \n",
    "train_input_ids = np.array(train_input_ids, dtype=int)\n",
    "train_attention_masks = np.array(train_attention_masks, dtype=int)\n",
    "train_token_type_ids = np.array(train_token_type_ids, dtype=int)\n",
    "train_labels = np.asarray(train_labels, dtype=int) #레이블 토크나이징 리스트\n",
    "train_inputs = (train_input_ids, train_attention_masks, train_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertNERClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertNERClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.num_class = num_class\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(self.num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                                name=\"ner_classifier\")\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "\n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output, training=training)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "ner_model = TFBertNERClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=len(ner_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    # -100의 레이블 값은 손실 값에서 제외 한다.\n",
    "#     active_loss = tf.reshape(labels, (-1,)) != -100\n",
    "    active_loss = tf.reshape(labels, (-1,)) != 0\n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n",
    "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "    \n",
    "    return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    return f1_pre_rec(labels, preds)\n",
    "\n",
    "def f1_pre_rec(labels, preds):\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds, suffix=True),\n",
    "        \"recall\": recall_score(labels, preds, suffix=True),\n",
    "        \"f1\": f1_score(labels, preds, suffix=True)\n",
    "    }\n",
    "\n",
    "\n",
    "def show_report(labels, preds):\n",
    "    return classification_report(labels, preds, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        results = {\n",
    "        }\n",
    "        \n",
    "        pred = self.model.predict(self.x_eval)\n",
    "        real = self.y_eval\n",
    "        preds = np.argmax(pred, axis = 2)\n",
    "\n",
    "        slot_label_map = {i: label for i, label in enumerate(ner_labels)}\n",
    "        out_label_ids = real\n",
    "\n",
    "        out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "        preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "        for i in range(out_label_ids.shape[0]):\n",
    "            for j in range(out_label_ids.shape[1]):\n",
    "                if out_label_ids[i, j] != 0:\n",
    "                    out_label_list[i].append(slot_label_map[out_label_ids[i][j]])\n",
    "                    preds_list[i].append(slot_label_map[preds[i][j]])\n",
    "                    \n",
    "        result = compute_metrics(out_label_list, preds_list)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"********\")\n",
    "        print(\"F1 Score\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"{}, {:.4f}\".format(key, results[key]))\n",
    "        print(\"\\n\" + show_report(out_label_list, preds_list))\n",
    "        print(\"********\")\n",
    "\n",
    "f1_score_callback = F1Metrics(train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "ner_model.compile(optimizer=optimizer, loss=compute_loss)\n",
    "# ner_model.compile(optimizer=optimizer, loss=compute_loss, metrics=[metric])\n",
    "# ner_model.compile(optimizer=optimizer, loss=compute_loss, metrics=[metric], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR/tf2_bert_ner -- Folder already exists \n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.8106e-04WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.9818\n",
      "precision, 0.9818\n",
      "recall, 0.9818\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       0.90      0.90      0.90        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       0.98      0.98      0.98        55\n",
      "macro avg       0.98      0.98      0.98        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 6s 1s/step - loss: 2.8106e-04\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 7.0768e-04WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 936ms/step - loss: 7.0768e-04\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0010    WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 961ms/step - loss: 0.0010\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0214   WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 989ms/step - loss: 0.0214\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0014  WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 978ms/step - loss: 0.0014\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0113WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 984ms/step - loss: 0.0113\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0171   WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 996ms/step - loss: 0.0171\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0057WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 910ms/step - loss: 0.0057\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0034WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 4s 934ms/step - loss: 0.0034\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0127WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 1.0000\n",
      "precision, 1.0000\n",
      "recall, 1.0000\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      CVL       1.00      1.00      1.00        10\n",
      "      DAT       1.00      1.00      1.00         2\n",
      "      LOC       1.00      1.00      1.00         2\n",
      "      PER       1.00      1.00      1.00        26\n",
      "      TIM       1.00      1.00      1.00         1\n",
      "      EVT       1.00      1.00      1.00         2\n",
      "      NUM       1.00      1.00      1.00         6\n",
      "      ORG       1.00      1.00      1.00         4\n",
      "      TRM       1.00      1.00      1.00         2\n",
      "\n",
      "micro avg       1.00      1.00      1.00        55\n",
      "macro avg       1.00      1.00      1.00        55\n",
      "\n",
      "********\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0127\n",
      "{'loss': [0.0002810613950714469, 0.0007076781475916505, 0.0010037899482995272, 0.021414943039417267, 0.0013854438439011574, 0.011346910148859024, 0.017060384154319763, 0.005667249672114849, 0.0033607613295316696, 0.012685305438935757]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_ner\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "# earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# history = ner_model.fit(train_inputs, train_labels, batch_size=8, epochs=10, validation_split = 0.2, callbacks=[earlystop_callback, cp_callback, f1_score])\n",
    "history = ner_model.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                        callbacks=[cp_callback, f1_score_callback])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"test.tsv\")\n",
    "\n",
    "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
    "\n",
    "test_ner_dict = {\"sentence\": test_sentences, \"label\": test_labels}\n",
    "test_ner_df = pd.DataFrame(test_ner_dict)\n",
    "\n",
    "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_token_type_ids = []\n",
    "test_labels = []\n",
    "\n",
    "for i, data in enumerate(test_ner_df[['sentence', 'label']].values):\n",
    "    sentence, labels = data\n",
    "    words = sentence.split()\n",
    "    labels = labels.split()\n",
    "        \n",
    "    labels_idx = []\n",
    "    for label in labels:\n",
    "        labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index(\"UNK\"))\n",
    "        \n",
    "    assert len(words) == len(labels_idx)\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids = bert_tokenizer(sentence, MAX_LEN)\n",
    "    \n",
    "    convert_label_ids = convert_label(words, labels_idx, MAX_LEN)\n",
    "        \n",
    "    test_input_ids.append(input_ids)\n",
    "    test_attention_masks.append(attention_mask)\n",
    "    test_token_type_ids.append(token_type_ids)\n",
    "    test_labels.append(convert_label_ids)\n",
    "    \n",
    "test_input_ids = np.array(test_input_ids, dtype=int)\n",
    "test_attention_masks = np.array(test_attention_masks, dtype=int)\n",
    "test_token_type_ids = np.array(test_token_type_ids, dtype=int)\n",
    "test_labels = np.asarray(test_labels, dtype=int) #레이블 토크나이징 리스트\n",
    "test_inputs = (test_input_ids, test_attention_masks, test_token_type_ids)\n",
    "\n",
    "result = model.evaluat(test_inputs, test_labels, batch_size=64)\n",
    "print(\"개체명 인식 테스트 결과 값 {}\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-compute-f1-score-for-named-entity-recognition-in-keras-6f28b31dccca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
