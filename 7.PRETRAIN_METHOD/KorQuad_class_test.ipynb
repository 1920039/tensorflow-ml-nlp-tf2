{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KorQuad_class.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMn6I90a+EqoM9Ks6eBcRWt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bc7f3c579a324f77811bdd6ad6dd7dc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e31de13423d743e68d6c451d23c93cdf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f8f80478dfca4894ac1ff8c2a082f734","IPY_MODEL_3be3c9704e934fb5a3d5847749d398ce"]}},"e31de13423d743e68d6c451d23c93cdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8f80478dfca4894ac1ff8c2a082f734":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2c0ecef646d44a0580cacefa5c3fd9f2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":871891,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":871891,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1fde406732df4b5b90b7701dc7e4981e"}},"3be3c9704e934fb5a3d5847749d398ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f58154a65f974e04bcf8af24b2884fdd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 872k/872k [00:00&lt;00:00, 3.17MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a7d4d0c48cda4abdb106a6bcfb24359e"}},"2c0ecef646d44a0580cacefa5c3fd9f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1fde406732df4b5b90b7701dc7e4981e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f58154a65f974e04bcf8af24b2884fdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a7d4d0c48cda4abdb106a6bcfb24359e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"TggNmT_jgGuY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":583},"executionInfo":{"status":"ok","timestamp":1594010748307,"user_tz":-540,"elapsed":8636,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"e5a417bd-c4b7-408c-ab71-16cae2bcd86f"},"source":["pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/25/89050e69ed53c2a3b7f8c67844b3c8339c1192612ba89a172cf85b298948/transformers-3.0.1-py3-none-any.whl (757kB)\n","\u001b[K     |████████████████████████████████| 757kB 3.5MB/s \n","\u001b[?25hCollecting tokenizers==0.8.0-rc4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 29.7MB/s \n","\u001b[?25hCollecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 38.4MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c484d24562e8b276624ee40834ce5943ce32337f92db5718595b19bbe82ec4bf\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B9WLyWEWgdDR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594010753269,"user_tz":-540,"elapsed":12607,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["import os\n","import re\n","import json\n","import string\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tokenizers import BertWordPieceTokenizer\n","from transformers import BertTokenizer, TFBertModel, BertConfig\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import matplotlib.pyplot as plt\n","import urllib\n","\n","max_len = 384\n","#configuration = BertConfig()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"68HVB3dYgi0w","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594010762115,"user_tz":-540,"elapsed":556,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["DATA_IN_PATH = 'data_in/KOR'\n","DATA_OUT_PATH = \"data_out/KOR\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvoswBdyglTQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594010763471,"user_tz":-540,"elapsed":639,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["def plot_graphs(history, string):\n","    plt.plot(history.history[string])\n","    plt.plot(history.history['var_'+_string, ''])\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(string)\n","    plt.legend([string, 'val_'+string])\n","    plt.show()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDI_cm3sgm6N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":65,"referenced_widgets":["bc7f3c579a324f77811bdd6ad6dd7dc0","e31de13423d743e68d6c451d23c93cdf","f8f80478dfca4894ac1ff8c2a082f734","3be3c9704e934fb5a3d5847749d398ce","2c0ecef646d44a0580cacefa5c3fd9f2","1fde406732df4b5b90b7701dc7e4981e","f58154a65f974e04bcf8af24b2884fdd","a7d4d0c48cda4abdb106a6bcfb24359e"]},"executionInfo":{"status":"ok","timestamp":1594010812799,"user_tz":-540,"elapsed":1217,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"33078a97-0007-428b-9439-b67bd53cd994"},"source":["# Save the slow pretrained tokenizer\n","slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n","save_path = \"bert-base-multilingual-uncased/\"\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","slow_tokenizer.save_pretrained(save_path)\n","\n","# Load the fast tokenizer from saved file\n","tokenizer = BertWordPieceTokenizer(\"bert-base-multilingual-uncased/vocab.txt\", lowercase=True)"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc7f3c579a324f77811bdd6ad6dd7dc0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"an5cGi-GgpG4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":83},"executionInfo":{"status":"ok","timestamp":1594010820826,"user_tz":-540,"elapsed":1750,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"c7753a24-f338-4a6d-8701-f78753f9b718"},"source":["train_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\"\n","train_path = keras.utils.get_file(\"train.json\", train_data_url)\n","eval_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\"\n","eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading data from https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n","38535168/38527475 [==============================] - 1s 0us/step\n","Downloading data from https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n","3883008/3881058 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v3iSyVdfcuaM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"ok","timestamp":1594010840395,"user_tz":-540,"elapsed":2011,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"6a8729fd-f199-4839-b917-8c71ce944880"},"source":["!wget -P ./bert-base-multilingual-uncased/ https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2020-07-06 04:47:19--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.105.133\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.105.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 625 [application/json]\n","Saving to: ‘./bert-base-multilingual-uncased/bert-base-multilingual-uncased-config.json’\n","\n","\r          bert-base   0%[                    ]       0  --.-KB/s               \rbert-base-multiling 100%[===================>]     625  --.-KB/s    in 0s      \n","\n","2020-07-06 04:47:19 (9.99 MB/s) - ‘./bert-base-multilingual-uncased/bert-base-multilingual-uncased-config.json’ saved [625/625]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"olsOtichcvHG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594010866573,"user_tz":-540,"elapsed":1700,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["!mv ./bert-base-multilingual-uncased/bert-base-multilingual-uncased-config.json ./bert-base-multilingual-uncased/config.json "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UCpszHHc5uN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"ok","timestamp":1594010913658,"user_tz":-540,"elapsed":23066,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"b976efed-cd09-45ce-9a17-357f73dba450"},"source":["!wget -P ./bert-base-multilingual-uncased/ https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-tf_model.h5"],"execution_count":10,"outputs":[{"output_type":"stream","text":["--2020-07-06 04:48:11--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-tf_model.h5\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.45\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.129.45|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 999358484 (953M) [binary/octet-stream]\n","Saving to: ‘./bert-base-multilingual-uncased/bert-base-multilingual-uncased-tf_model.h5’\n","\n","bert-base-multiling 100%[===================>] 953.06M  39.2MB/s    in 21s     \n","\n","2020-07-06 04:48:33 (44.6 MB/s) - ‘./bert-base-multilingual-uncased/bert-base-multilingual-uncased-tf_model.h5’ saved [999358484/999358484]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UFOJPxyic5yG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594010914796,"user_tz":-540,"elapsed":14933,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["!mv ./bert-base-multilingual-uncased/bert-base-multilingual-uncased-tf_model.h5 ./bert-base-multilingual-uncased/tf_model.h5"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkuK7N_ngrMd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594011009085,"user_tz":-540,"elapsed":99893,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"48275df3-52de-4623-dfc3-db6be9a54dfa"},"source":["class SquadExample:\n","    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n","        self.question = question\n","        self.context = context\n","        self.start_char_idx = start_char_idx\n","        self.answer_text = answer_text\n","        self.all_answers = all_answers\n","        self.skip = False\n","\n","    def preprocess(self):\n","        context = self.context\n","        question = self.question\n","        answer_text = self.answer_text\n","        start_char_idx = self.start_char_idx\n","\n","        # Clean context, answer and question\n","        context = \" \".join(str(context).split())\n","        question = \" \".join(str(question).split())\n","        answer = \" \".join(str(answer_text).split())\n","\n","        # Find end character index of answer in context\n","        end_char_idx = start_char_idx + len(answer)\n","        if end_char_idx >= len(context):\n","            self.skip = True\n","            return\n","\n","        # Mark the character indexes in context that are in answer\n","        is_char_in_ans = [0] * len(context)\n","        for idx in range(start_char_idx, end_char_idx):\n","            is_char_in_ans[idx] = 1\n","\n","        # Tokenize context\n","        tokenized_context = tokenizer.encode(context)\n","        #print(tokenized_context)\n","        # Find tokens that were created from answer characters\n","        ans_token_idx = []\n","        for idx, (start, end) in enumerate(tokenized_context.offsets):\n","            if sum(is_char_in_ans[start:end]) > 0:\n","                ans_token_idx.append(idx)\n","\n","        if len(ans_token_idx) == 0:\n","            self.skip = True\n","            return\n","\n","        # Find start and end token index for tokens from answer\n","        start_token_idx = ans_token_idx[0]\n","        end_token_idx = ans_token_idx[-1]\n","\n","        # Tokenize question\n","        tokenized_question = tokenizer.encode(question)\n","\n","        # Create inputs\n","        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n","        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n","            tokenized_question.ids[1:]\n","        )\n","        attention_mask = [1] * len(input_ids)\n","\n","        # Pad and create attention masks.\n","        # Skip if truncation is needed\n","        padding_length = max_len - len(input_ids)\n","        if padding_length > 0:  # pad\n","            input_ids = input_ids + ([0] * padding_length)\n","            attention_mask = attention_mask + ([0] * padding_length)\n","            token_type_ids = token_type_ids + ([0] * padding_length)\n","        elif padding_length < 0:  # skip\n","            self.skip = True\n","            return\n","\n","        self.input_ids = input_ids\n","        self.token_type_ids = token_type_ids\n","        self.attention_mask = attention_mask\n","        self.start_token_idx = start_token_idx\n","        self.end_token_idx = end_token_idx\n","        self.context_token_to_char = tokenized_context.offsets\n","\n","\n","with open(train_path) as f:\n","    raw_train_data = json.load(f)\n","\n","with open(eval_path) as f:\n","    raw_eval_data = json.load(f)\n","\n","\n","def create_squad_examples(raw_data):\n","    squad_examples = []\n","    for item in raw_data[\"data\"]:\n","        for para in item[\"paragraphs\"]:\n","            context = para[\"context\"]\n","            for qa in para[\"qas\"]:\n","                question = qa[\"question\"]\n","                answer_text = qa[\"answers\"][0][\"text\"]\n","                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n","                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n","                squad_eg = SquadExample(\n","                    question, context, start_char_idx, answer_text, all_answers\n","                )\n","                squad_eg.preprocess()\n","                squad_examples.append(squad_eg)\n","    return squad_examples\n","\n","\n","def create_inputs_targets(squad_examples):\n","    dataset_dict = {\n","        \"input_ids\": [],\n","        \"token_type_ids\": [],\n","        \"attention_mask\": [],\n","        \"start_token_idx\": [],\n","        \"end_token_idx\": [],\n","    }\n","    for item in squad_examples:\n","        if item.skip == False:\n","            for key in dataset_dict:\n","                dataset_dict[key].append(getattr(item, key))\n","    for key in dataset_dict:\n","        dataset_dict[key] = np.array(dataset_dict[key])\n","\n","    x = [\n","        dataset_dict[\"input_ids\"],\n","        dataset_dict[\"token_type_ids\"],\n","        dataset_dict[\"attention_mask\"],\n","    ]\n","    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n","    return x, y\n","\n","\n","train_squad_examples = create_squad_examples(raw_train_data)\n","x_train, y_train = create_inputs_targets(train_squad_examples)\n","print(f\"{len(train_squad_examples)} training points created.\")\n","\n","eval_squad_examples = create_squad_examples(raw_eval_data)\n","x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n","print(f\"{len(eval_squad_examples)} evaluation points created.\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["60407 training points created.\n","5774 evaluation points created.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mIjk3_XeguBj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594011009787,"user_tz":-540,"elapsed":690,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["class TFBERTKorquad(tf.keras.Model):\n","    def __init__(self, max_len=None):\n","        super(TFBERTKorquad, self).__init__()\n","        self.encoder = TFBertModel.from_pretrained('./bert-base-multilingual-uncased/')\n","        #self.encoder = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n","        self.start_logit = tf.keras.layers.Dense(1, name=\"start_logit\", use_bias=False)\n","        self.end_logit = tf.keras.layers.Dense(1, name=\"end_logit\", use_bias=False)\n","        self.flatten = tf.keras.layers.Flatten() \n","        \n","        \n","    def call(self, inputs):\n","        input_ids, token_type_ids, attention_mask = inputs\n","        embedding = self.encoder(inputs, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n","        start_logits = self.start_logit(embedding)\n","        start_logits = self.flatten(start_logits)\n","\n","        end_logits = self.end_logit(embedding)\n","        end_logits = self.flatten(end_logits)\n","        \n","        start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(start_logits)\n","        end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(end_logits)\n","    \n","        return start_probs, end_probs"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4t_2T7vgwOu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"status":"ok","timestamp":1594011020239,"user_tz":-540,"elapsed":11135,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"fd7dcb5d-bf36-496c-b53d-53e89962360a"},"source":["korquad_model = TFBERTKorquad(max_len)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at ./bert-base-multilingual-uncased/ were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the model checkpoint at ./bert-base-multilingual-uncased/.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YZtVFA3PgyL0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594011103474,"user_tz":-540,"elapsed":590,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["def normalize_answer(s):    \n","    def remove_(text):\n","        ''' 불필요한 기호 제거 '''\n","        text = re.sub(\"'\", \" \", text)\n","        text = re.sub('\"', \" \", text)\n","        text = re.sub('《', \" \", text)\n","        text = re.sub('》', \" \", text)\n","        text = re.sub('<', \" \", text)\n","        text = re.sub('>', \" \", text) \n","        text = re.sub('〈', \" \", text)\n","        text = re.sub('〉', \" \", text)   \n","        text = re.sub(\"\\(\", \" \", text)\n","        text = re.sub(\"\\)\", \" \", text)\n","        text = re.sub(\"‘\", \" \", text)\n","        text = re.sub(\"’\", \" \", text)      \n","        return text\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_punc(lower(remove_(s))))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVTh1qKng1p8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594011104061,"user_tz":-540,"elapsed":720,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["class ExactMatch(keras.callbacks.Callback):\n","    \"\"\"\n","    Each `SquadExample` object contains the character level offsets for each token\n","    in its input paragraph. We use them to get back the span of text corresponding\n","    to the tokens between our predicted start and end tokens.\n","    All the ground-truth answers are also present in each `SquadExample` object.\n","    We calculate the percentage of data points where the span of text obtained\n","    from model predictions matches one of the ground-truth answers.\n","    \"\"\"\n","\n","    def __init__(self, x_eval, y_eval):\n","        self.x_eval = x_eval\n","        self.y_eval = y_eval\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        pred_start, pred_end = self.model.predict(self.x_eval)\n","        count = 0\n","        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n","        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n","            squad_eg = eval_examples_no_skip[idx]\n","            offsets = squad_eg.context_token_to_char\n","            start = np.argmax(start)\n","            end = np.argmax(end)\n","            if start >= len(offsets):\n","                continue\n","            pred_char_start = offsets[start][0]\n","            if end < len(offsets):\n","                pred_char_end = offsets[end][1]\n","                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n","            else:\n","                pred_ans = squad_eg.context[pred_char_start:]\n","\n","            normalized_pred_ans = normalize_answer(pred_ans)\n","            normalized_true_ans = [normalize_answer(_) for _ in squad_eg.all_answers]\n","            if normalized_pred_ans in normalized_true_ans:\n","                count += 1\n","        acc = count / len(self.y_eval[0])\n","        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTgvtk0og4Ow","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594011104303,"user_tz":-540,"elapsed":399,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["exact_match_callback = ExactMatch(x_eval, y_eval)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EuBYS58g6QZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594011105561,"user_tz":-540,"elapsed":599,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}}},"source":["korquad_model.compile(optimizer=optimizer, loss=[loss, loss])"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZehxFPSrg8Q2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594011106252,"user_tz":-540,"elapsed":714,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"6a33f8a1-84d0-48c4-ac1e-5843daf1f2fb"},"source":["model_name = \"tf2_bert_korquad\"\n","\n","# overfitting을 막기 위한 ealrystop 추가\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n","\n","checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create path if exists\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","    \n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["data_out/KOR/tf2_bert_korquad -- Folder create complete \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ljuajCLmyws","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"ok","timestamp":1594029233934,"user_tz":-540,"elapsed":18126376,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"e89526e8-e795-48df-eead-1a00b28005bf"},"source":["history = korquad_model.fit(\n","    x_train,\n","    y_train,\n","    epochs=3,  # For demonstration, 3 epochs are recommended\n","    verbose=2,\n","    batch_size=4, #2->64\n","    callbacks=[exact_match_callback, earlystop_callback, cp_callback]\n",")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","\n","epoch=1, exact match score=0.06\n","WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss\n","WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","7788/7788 - 6010s - loss: 7.6960 - output_1_loss: 3.7383 - output_2_loss: 3.9577\n","Epoch 2/3\n","\n","epoch=2, exact match score=0.06\n","WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss\n","WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","7788/7788 - 6049s - loss: 6.7806 - output_1_loss: 3.4303 - output_2_loss: 3.3503\n","Epoch 3/3\n","\n","epoch=3, exact match score=0.07\n","WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss\n","WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","7788/7788 - 6043s - loss: 6.1601 - output_1_loss: 3.2470 - output_2_loss: 2.9131\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vw7yLoddm2Px","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"error","timestamp":1594029235888,"user_tz":-540,"elapsed":18125025,"user":{"displayName":"ChangWook Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64","userId":"00685987924881157185"}},"outputId":"c2ec1133-1486-430c-9323-1101d617426b"},"source":["plot_graphs(history, 'accuracy')\n","plot_graphs(history, 'loss')"],"execution_count":21,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-b5e0e0edac04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-2d751623cd0e>\u001b[0m in \u001b[0;36mplot_graphs\u001b[0;34m(history, string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'var_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0m_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'accuracy'"]}]},{"cell_type":"code","metadata":{"id":"QxaigHy2m4JB","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}